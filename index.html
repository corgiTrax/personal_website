
<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111713571-1"></script>
	<script>
  		window.dataLayer = window.dataLayer || [];
  		function gtag(){dataLayer.push(arguments);}
  		gtag('js', new Date());
  		gtag('config', 'UA-111713571-1');
	</script>

	<!-- Show more content -->
	<script type="text/javascript">
		function toggle_vis(id) {
	    // var e = document.getElementById(id);
	    var e = document.getElementsByClassName(id);
			var showText = document.getElementById("showText");
			for (var i = 0; i < e.length; i++) {
		    	if (e[i].style.display == "none") {
		        	e[i].style.display = "inline";
	    			showText.innerHTML = "[Show less]";
		    	} else {
		    		e[i].style.display = "none";
		    		showText.innerHTML = "[Show more]";
		    	}
		    }
	    }
	</script>

	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<!-- https://fontawesome.com/cheatsheet -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
  <link rel="stylesheet" href="static/styles.css">
	<!-- Custom styles for this template -->
	<link href="files/jumbotron.css" rel="stylesheet">
	<script src="js/main.js"></script>
  <script src="js/scroll.js"></script>
</head>

<title>Ruohan Zhang</title>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark" id="Home">
	    <div class="container">
		<a class="navbar-brand" href="#Home">Ruohan Zhang</a>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="#Home">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Publications">Publications</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Awards">Awards</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Service">Service and Teaching</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Mentoring">Mentoring</a>
				</li>
			</ul>
		</div>
	    </div>
	</nav>

	<div class="container" style="padding-top: 30px; font-size: 17px">
		<div class="row">
			<div class="col-md-3", style="padding-right: 40px">
				<br>
				<img class="img-responsive img-rounded" src="images/profile.jpg" alt="Photo" style="max-width: 100%; border:0px solid black"><br>
			</div>

			<div class="col-md-9">
			<br>
			<p><font style="font-size: 20px"><b>Ruohan Zhang</b></font></p>

            <p>I am a postdoctoral researcher at <a href="http://svl.stanford.edu/">Stanford Vision and Learning Lab (SVL)</a>, <a href="https://hai.stanford.edu/">Stanford Institute for Human-Centered Artificial Intelligence (HAI)</a>, and <a href="https://humanperformancealliance.org/">Wu Tsai Human Performance Alliance</a>. I organize the <a href="https://pair.stanford.edu/">Stanford People, AI & Robots Group</a>. I work on robotics, human-robot interaction, brain-machine interface, neuroscience, and art.
              I am currently working with <a href="http://svl.stanford.edu/people/">Prof. Fei-Fei Li, Prof. Jiajun Wu, and Prof. Silvio Savarese</a>. I received my Ph.D. from the University of Texas at Austin, advised by <a href="http://www.cs.utexas.edu/~dana/">Prof. Dana Ballard</a> and <a href="http://www.utexas.edu/cola/centers/cps/faculty/mmh739">Prof. Mary Hayhoe</a>. 
            </p>
            <p>
            <font color="firebrick"><strong>I will be joining Northwestern University in Fall 2026. </strong></font>
            </p>

			<p>
			<a target="_blank" href="https://scholar.google.com/citations?user=-bqvNWoAAAAJ&hl=en&oi=ao"><font color="black"><i class="ai ai-google-scholar ai-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
			<!-- <a target="_blank" href="https://github.com/corgiTrax"><font color="black"><i class="fab fa-github fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp; -->
			<a target="_blank" href="https://twitter.com/RuohanZhang76"><font color="black"><i class="fab fa-twitter fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
			zharu [at] stanford (dot) edu
			</p>

			</div>
		</div>
	</div><br><br>


	<div class="container">
		<h3 id="Research" style="padding-top: 80px; margin-top: -80px;">Research: Human-Centered AI and Robotics</h3>
		
                My longterm research interest is human-centered artificial intelligence and robotics, inspired by <a href="https://hai.stanford.edu/">Stanford Institute for Human-Centered Artificial Intelligence (HAI)</a>'s guidelines:
				<ul>
					<li>
					<font color="firebrick"><b>Human Impact</b></font>:
					Studying, forecasting, and guiding the human and societal impact of AI and robotics. 
					We seek to identify tasks that matter to humans, and develop benchmarks and datasets to guide our research toward solving these tasks, as in
					<a href="https://behavior.stanford.edu/"><b>BEHAVIOR</b></a> |
					<a href="https://embodied-agent-interface.github.io/">BEHAVIOR Vision Suite</a> |
					<a href="https://embodied-agent-interface.github.io/">Embodied Agent Interface</a>
					<!-- ACDC | Econ, Elderly, robotRubric | -->
					</li>
					<li>
					<font color="firebrick"><b>Human Intelligence</b></font>:
					Developing human-level, human-inspired intelligence. 
					We leverage large-scale and diverse human data for robot learning, as in
					<a href="https://behavior-robot-suite.github.io/"><b>BEHAVIOR Robot Suite</b></a> | 
					<a href="https://mimic-play.github.io/"><b>MimicPlay</b></a> | 
					<a href="https://dex-cap.github.io/"><b>DexCap</b></a> | 
					<a href="https://transic-robot.github.io/"><b>TRANSIC</b></a> |
					<a href="https://seediros23.github.io/">SEED</a> |
					<a href="https://aaai.org/ojs/index.php/AAAI/article/view/6161/">AGIL</a>.
					<!--RTX-->
					<br>
					We further try to better understand human brain and behaviors (neuroscience and cognitive science), as in
					<a href="https://direct.mit.edu/neco/article/32/9/1635/95604/Parallel-Neural-Multiprocessing-with-Gamma">GSM</a> |
					<a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006518&rev=2">ModularIRL</a> |
					<a href="https://sites.google.com/view/vi-vr/?authuser=1">VIVR</a>.
					<!--MARPLE, <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/d58e2f077670f4de9cd7963c857f2534-Paper.pdf">MvHA</a>-->
					</li> 
					<li>
					<font color="firebrick"><b>Human Augmentation</b></font>:
					Creating AI and robotic applications that augment human capabilities. 
					We aim at widening the communication pipeline between humans and robots, as in
					<a href="https://noir-corl.github.io/"><b>NOIR</b></a> |
					<a href="https://voxposer.github.io/"><b>VoxPoser</b></a> |
					<a href="https://rekep-robot.github.io/">ReKep</a>
					<!--NOIR-implant, HRAlignment-->
					</li> 
				</ul>
                	<!-- 5. Foundation model for robotics
                	RoboArt, UAD, Rekep-->

	</div><br>


	<!-- Publications -->
	<div class="container">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">
			Publications
			<small><small>
			(<a href="" id="select0" onclick="showPubs(0); return false;">show selected</a> /
			 <a href="" id="select1" onclick="showPubs(1); return false;">show by date</a> /
       		 <a href="" id="select2" onclick="showPubs(2); return false;">show by topic</a>)
			</small></small><br>

			<small><small>
		  	<font color="black">Research Topics:
				<a href="#db" onclick="showPubs(2)">Datasets and Benchmarks</a> /
				<a href="#lfh" onclick="showPubs(2)">Robot Learning from Humans</a> /
				<a href="#hri" onclick="showPubs(2)">Human-Robot Interaction</a> /
				<a href="#fm" onclick="showPubs(2)">Foundation Models for Robotics</a> /
				<a href="#ns" onclick="showPubs(2)">Neuroscience and Cognitive Science</a>
			</font><br>
			</small></small>
		</h3>

		<div id="pubs"></div>

		<script id="pubs_selected" language="text">
			<font color="black">(*† indicates equal contribution)</font><br><hr>


			<div class="row">
				<div class="col-md-3">
					<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/uad.mp4" type="video/mp4">
						</video>
					</div>
				<div class="col-md-9">
				<b><font color="black">UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation</font></b><br>
				<a href="https://tangyihe.com/" target="_blank">Yihe Tang</a>, 
				<a href="https://wenlong.page/" target="_blank">Wenlong Huang</a>,
				<a href="https://www.wykac.com/" target="_blank">Yingke Wang</a>,
				<a href="https://www.chengshuli.me/" target="_blank">Chengshu Li</a>,
				<a href="https://www.linkedin.com/in/ryuan19/" target="_blank">Roy Yuan</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="https://2025.ieee-icra.org/" target="_blank">International Conference on Robotics and Automation (ICRA), 2025</a></b>
				<br>
				<font color="firebrick"><b>Best Paper Award Finalist</b></font> 
				<br>
				<font color="firebrick"><b>Best Paper Award Finalist in Robot Perception</b></font> 
				<br>
				<a href="https://x.com/RuohanZhang76/status/1924489528007426119">tl;dr</a> |
				<a href="https://arxiv.org/abs/2506.09284">paper</a> |
				<a href="https://unsup-affordance.github.io/">website</a> |
				<a href="https://github.com/TangYihe/unsup-affordance">code</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/brs.mp4" type="video/mp4">
						</video>
					</div>
				<div class="col-md-9">
				<b><font color="black">BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities</font></b><br>
				<a href="https://yunfanj.com/" target="_blank">Yunfan Jiang</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="https://jdw.ong/" target="_blank">Josiah Wong</a>,  
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>,
				<a href="https://yanjieze.com/" target="_blank">Yanjie Ze</a>,
				<a href="https://www.linkedin.com/in/hangyin0226/" target="_blank">Hang Yin</a>,
				<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>, 
				<a href="https://shurans.github.io/" target="_blank">Shuran Song</a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="" target="_blank">arxiv 2025</a></b>
				<br>
				<a href="https://x.com/RuohanZhang76/status/1900328330831991050">tl;dr</a> |
				<a href="https://arxiv.org/abs/2503.05652">paper</a> |
				<a href="https://behavior-robot-suite.github.io/">website</a> |
				<a href="https://behavior-robot-suite.github.io/docs/">documentation</a> |
				<a href="https://github.com/behavior-robot-suite/brs-ctrl">robot code</a> |
				<a href="https://github.com/behavior-robot-suite/brs-algo">algorithm code</a> 
				</div>
			</div><hr>


			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2024-eai.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
					</div>
				<div class="col-md-9">
				<b><font color="black">Embodied Agent Interface: A Single Line to Evaluate LLMs for Embodied Decision Making</font></b><br>
				<a href="https://limanling.github.io/">Manling Li</a>*, 
				Shiyu Zhao*, 
				<a href="https://qinengwang-aiden.github.io/">Qineng Wang</a>*, 
				<a href="https://jameskrw.github.io/">Kangrui Wang</a>*, 
				<a href="https://bryanzhou008.github.io/">Yu Zhou</a>*, 
				<a href="https://www.linkedin.com/in/sanjana-srivastava5/" target="_blank">Sanjana Srivastava</a>, 
				<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>, 
				<a href="https://www.linkedin.com/in/tonyhlee/">Tony Lee</a>, 
				<a href="https://www.cs.columbia.edu/~lierranli/">Li Erran Li</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="http://weiyuliu.com/" target="_blank">Weiyu Liu</a>, 
				<a href="https://cs.stanford.edu/~pliang/">Percy Liang</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>,
				<a href="https://jiayuanm.com/" target="_blank">Jiayuan Mao</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><a href="https://neurips.cc/" target="_blank">NeurIPS Datasets and Benchmarks Track 2024</a></b>
				<br>
				<font color="firebrick"><b>Oral Presentation</b></font> 
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1855401520793264302">tl;dr</a> |
				<a href="https://arxiv.org/abs/2410.07166">paper</a> |
				<a href="https://embodied-agent-interface.github.io/">website</a> |
				<a href="https://github.com/embodied-agent-interface/embodied-agent-interface">code</a> 
				</div>
			</div><hr>


			<div class="row">
				<div class="col-md-3">
					<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/rekep.mp4" type="video/mp4">
						</video>
					</div>
				<div class="col-md-9">
				<b><font color="black">ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation</font></b><br>
				<a href="https://wenlong.page/" target="_blank">Wenlong Huang</a>,
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>*,
				<a href="https://yunzhuli.github.io/">Yunzhu Li</a>*,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="https://www.corl.org/" target="_blank">Conference on Robot Learning (CoRL) 2024</a></b>
				<br>
				<font color="firebrick"><b>Best Paper Award</b></font> at
				<b><a href="https://leap-workshop.github.io/" target="_blank">Learning Effective Abstractions for Planning (LEAP) Workshop at CoRL 2024</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1830534553267782005">tl;dr</a> |
				<a href="https://arxiv.org/abs/2409.01652">paper</a> |
				<a href="https://rekep-robot.github.io/">website</a> |
				<a href="https://github.com/huangwl18/ReKep">code</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/transic.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction</font></b><br>
				<a href="https://yunfanj.com/" target="_blank">Yunfan Jiang</a>, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="https://www.corl.org/" target="_blank">Conference on Robot Learning (CoRL) 2024</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1791603579159269835">tl;dr</a> |
				<a href="https://arxiv.org/abs/2405.10315">paper</a> |
				<a href="https://transic-robot.github.io/">website</a> |
				<a href="https://github.com/transic-robot/transic">code</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/acdc.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Automated Creation of Digital Cousins for Robust Policy Learning</font></b><br>
				<a href="https://rogerdai1217.github.io/" target="_blank">Tianyuan Dai</a>,  
				<a href="https://jdw.ong/" target="_blank">Josiah Wong</a>,  
				<a href="https://yunfanj.com/" target="_blank">Yunfan Jiang</a>, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="https://www.corl.org/" target="_blank">Conference on Robot Learning (CoRL) 2024</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1844448510864916511">tl;dr</a> |
				<a href="https://arxiv.org/abs/2410.07408">paper</a> |
				<a href="https://digital-cousins.github.io/">website</a> |
				<a href="https://github.com/cremebrule/digital-cousins">code</a> 
				</div>
			</div><hr>
			
			<div class="row">
				<div class="col-md-3">
					<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/dexcap.mp4" type="video/mp4">
					</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation</font></b><br>
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a>, 
				<a href="https://www.linkedin.com/in/weizhuo-ken-wang-ba4921119/" target="_blank">Weizhuo Wang</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://tml.stanford.edu/people/karen-liu" target="_blank">C. Karen Liu</a>
				<br>
				<b><a href="https://roboticsconference.org/" target="_blank">Robotics: Science and Systems (RSS) 2024</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1769533513940738127">tl;dr</a> |
				<a href="https://arxiv.org/abs/2403.07788">paper</a> |
				<a href="https://dex-cap.github.io/">website</a> |
				<a href="https://docs.google.com/document/d/1ANxSA_PctkqFf3xqAkyktgBgDWEbrFK7b1OnJe54ltw/edit#heading=h.yxlxo67jgfyx">hardware</a> |
				<a href="https://github.com/j96w/DexCap">code</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-left: 30pt; width: 70%; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/bvs.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation</font></b><br>
				<a href="https://gyhandy.github.io/" target="_blank">Yunhao Ge</a>*, 
				<a href="https://tangyihe.com/" target="_blank">Yihe Tang</a>*, 
				<a href="https://cnut1648.github.io/" target="_blank">Jiashu Xu</a>*, 
				<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>*, 
				<a href="https://www.chengshuli.me/" target="_blank">Chengshu Li</a>,
				<a href="https://wensi-ai.github.io/" target="_blank">Wensi Ai</a>,
				<a href="https://web.stanford.edu/~benjm/" target="_blank">Benjamin Jose Martinez</a>,
				<a href="https://www.linkedin.com/in/arman-aydin/" target="_blank">Arman Aydin</a>,
				<a href="https://www.linkedin.com/in/mona-anvari/" target="_blank">Mona Anvari</a>,
				<a href="https://www.linkedin.com/in/ayush-chakravarthy/" target="_blank">Ayush K Chakravarthy</a>,
				<a href="https://kovenyu.com/" target="_blank">Hong-Xing Yu</a>,
				<a href="https://jdw.ong/" target="_blank">Josiah Wong</a>, 
				<a href="https://www.linkedin.com/in/sanjana-srivastava5/" target="_blank">Sanjana Srivastava</a>, 
				<a href="https://knight-hennessy.stanford.edu/people/sharon-lee" target="_blank">Sharon Lee</a>, 
				<a href="https://www.linkedin.com/in/shengxin-cindy-z-4080b525/" target="_blank">Shengxin Zha</a>, 
				<a href="http://ilab.usc.edu/itti/" target="_blank">Laurent Itti</a>, 
				<a href="https://yunzhuli.github.io/">Yunzhu Li</a>, 
				<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>, 
				<a href="https://aptx4869lm.github.io/">Miao Liu</a>, 
				<a href="https://pzzhang.github.io/pzzhang/">Pengchuan Zhang</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a> 
				<br>
				<b><a href="https://2024.ieee-icra.org/" target="_blank">Conference on Computer Vision and Pattern Recognition (CVPR) 2024</a></b>
				<br>
				<font color="firebrick"><b>Highlight</b></font> 
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1802470520493146214">tl;dr</a> |
          		<a href="https://arxiv.org/abs/2405.09546">paper</a> |
          		<a href="https://behavior-vision-suite.github.io/">website</a> |
          		<a href="https://github.com/behavior-vision-suite/behavior-vision-suite.github.io">code</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 10pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/noir.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">NOIR: Neural Signal Operated Intelligent Robots for Everyday Activities</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>*, 
				<a href="https://knight-hennessy.stanford.edu/people/sharon-lee" target="_blank">Sharon Lee</a>*, 
				<a href="https://mj-hwang.github.io/" target="_blank">Minjune Hwang</a>*,
				<a href="https://misoshiruseijin.github.io/" target="_blank">Ayano Hiranaka</a>*, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://wensi-ai.github.io/" target="_blank">Wensi Ai</a>, 
				<a href="https://www.linkedin.com/in/ryantjj/" target="_blank">Jin Jie Ryan Tan</a>,
				<a href="https://www.linkedin.com/in/shreya-gupta-08/" target="_blank">Shreya Gupta</a>, 
				<a href="https://yih301.github.io/" target="_blank">Yilun Hao</a>, 
				<a href="https://www.gabrael.io/" target="_blank">Gabrael Levine</a>, 
				<a href="https://ruohangao.github.io/" target="_blank">Ruohan Gao</a>, 
				<a href="https://psychology.stanford.edu/people/anthony-norcia" target="_blank">Anthony Norcia</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><a href="https://www.corl2023.org/" target="_blank">Conference on Robot Learning (CoRL) 2023</a></b>
				<br>
				<b>
				<font color="firebrick"><b>Oral Presentation</b></font> at
				<a href="https://yantianzha.github.io/crl.github.io/" target="_blank">Bridging the Gap between Cognitive Science and Robot Learning Workshop at CoRL 2023</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1720525179028406492">tl;dr</a> |
				<a href="https://arxiv.org/abs/2311.01454">paper</a> |
				<a href="https://noir-corl.github.io/">website</a> | 
				<a href="https://hai.stanford.edu/news/wearable-device-allows-humans-control-robots-brain-waves">Stanford HAI News</a> |
				<a href="https://w.mgtv.com/b/607797/20399189.html?t=videoshare&externalsource=v_play&tc=jXKKosRPSAN7&f=wxf&dc=9c199a33-461e-488e-bc06-3b252fe3e950">MangoTV interview (湖南卫视专访)</a> |
				<a href="https://www.deeplearning.ai/the-batch/issue-249/">The Batch</a> |
				<a href="https://braintitan.medium.com/noir-brain-powered-robots-for-daily-tasks-2cebd6ff6046">Medium</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/mimicplay.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Mimicplay: Long-horizon Imitation Learning by Watching Human Play</font></b><br>
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://jimfan.me/" target="_blank">Linxi Fan</a>, 
				<a href="https://web.stanford.edu/~jksun/" target="_blank">Jiankai Sun</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://faculty.cc.gatech.edu/~danfei/">Danfei Xu</a>, 
				<a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu</a>, 
				<a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a> 
				<br>
				<b><a href="https://www.corl2023.org/" target="_blank">Conference on Robot Learning (CoRL) 2023</a></b>
				<br>
				<font color="firebrick"><b>Finalist - Best Paper/Best Student Paper Awards <br>
					Finalist - Best Systems Paper Award <br> Oral Presentation</b></font> 
				<br>
				<a href="https://twitter.com/chenwang_j/status/1628792565385564160">tl;dr</a> |
				<a href="https://arxiv.org/abs/2302.12422">paper</a> |
				<a href="https://mimic-play.github.io/">website</a> |
				<a href="https://github.com/j96w/MimicPlay">code</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/voxposer.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models</font></b><br>
				<a href="https://wenlong.page/" target="_blank">Wenlong Huang</a>, <a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, <a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, <a href="https://yunzhuli.github.io/">Yunzhu Li</a>, <a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, <a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="https://www.corl2023.org/" target="_blank">Conference on Robot Learning (CoRL) 2023</a></b>
				<br>
				<font color="firebrick"><b>Oral Presentation</b></font> 
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1677376310811959296">tl;dr</a> |
				<a href="https://arxiv.org/abs/2307.05973">paper</a> |
				<a href="https://voxposer.github.io/">website</a> |
				<a href="https://github.com/huangwl18/VoxPoser">code</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 40pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/behavior.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation</font></b><br>
				<a href="https://www.chengshuli.me/" target="_blank">Chengshu Li</a>*, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>*, 
				<a href="https://jdw.ong/" target="_blank">Josiah Wong</a>*, 
				<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>*, 
				<a href="https://www.linkedin.com/in/sanjana-srivastava5/" target="_blank">Sanjana Srivastava</a>*, 
				<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>*, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>*, 
				<a href="https://www.gabrael.io/" target="_blank">Gabrael Levine</a>*, 
				<a href="https://wensi-ai.github.io/" target="_blank">Wensi Ai</a>*, 
				<a href="https://web.stanford.edu/~benjm/" target="_blank">Benjamin Jose Martinez</a>,
				<a href="https://www.linkedin.com/in/hangyin0226/" target="_blank">Hang Yin</a>,
				<a href="https://mlingelbach.com/" target="_blank">Michael Lingelbach</a>,
				<a href="https://mj-hwang.github.io/" target="_blank">Minjune Hwang</a>,
				<a href="https://misoshiruseijin.github.io/" target="_blank">Ayano Hiranaka</a>, 
				<a href="https://sujaygarlanka.com/" target="_blank">Sujay Garlanka</a>, 
				<a href="https://www.linkedin.com/in/arman-aydin/" target="_blank">Arman Aydin</a>, 
				<a href="https://knight-hennessy.stanford.edu/people/sharon-lee" target="_blank">Sharon Lee</a>, 
				<a href="https://web.stanford.edu/~jksun/" target="_blank">Jiankai Sun</a>,
				<a href="https://www.linkedin.com/in/mona-anvari/" target="_blank">Mona Anvari</a>,
				<a href="https://manasi-sharma.github.io/" target="_blank">Manasi Sharma</a>,
				<a href="https://www.linkedin.com/in/dhruvabansal2k/" target="_blank">Dhruva Bansal</a>,
				<a href="" target="_blank">Samuel Hunter</a>,
				<a href="https://kykim0.github.io/">Kyu-Young Kim</a>, 
				<a href="https://www.linkedin.com/in/alanlou/">Alan Lou</a>, 
				<a href="https://www.linkedin.com/in/caleb-matthews/">Caleb R Matthews</a>, 
				<a href="https://ivillar.github.io/">Ivan Villa-Renteria</a>, 
				<a href="https://www.linkedin.com/in/jerry-tang-b37026162/">Jerry Huayang Tang</a>, 
				<a href="">Claire Tang</a>, 
				<a href="https://fxia22.github.io/">Fei Xia</a>, 
				<a href="https://yunzhuli.github.io/">Yunzhu Li</a>, 
				<a href="https://www.linkedin.com/in/silvio-savarese-97b76114/">Silvio Savarese</a>, 
				<a href="https://web.stanford.edu/~hyo/Home.html">Hyowon Gweon</a>, 
				<a href="https://tml.stanford.edu/people/karen-liu" target="_blank">C. Karen Liu</a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>     
				<br>
				<b><a href="https://corl2022.org/" target="_blank">Conference on Robot Learning (CoRL) 2022</a></b>
				<br>
				<font color="firebrick"><b>Best Paper Nomination</b></font> 
				<br>
				<font color="firebrick"><b>Oral Presentation</b></font> 
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1771019123298066812">tl;dr</a> |
				<a href="https://arxiv.org/abs/2403.09227">paper</a> |
				<a href="https://behavior.stanford.edu/">website</a> |
				<a href="https://github.com/StanfordVL/OmniGibson">code</a> |
				<a href="https://behavior.stanford.edu/omnigibson/getting_started/installation.html">guide</a> |
				<a href="https://discord.com/invite/bccR5vGFEx">support</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/dr-hrl.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">A Dual Representation Framework for Robot Learning with Human Guidance</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>*, 
				<a href="https://www.linkedin.com/in/dhruvabansal2k/" target="_blank">Dhruva Bansal</a>*, 
				<a href="https://yih301.github.io/" target="_blank">Yilun Hao</a>*, 
				<a href="https://misoshiruseijin.github.io/" target="_blank">Ayano Hiranaka</a>, 
				<a href="https://gaojl19.github.io/">Jialu Gao</a>, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><a href="https://corl2022.org/" target="_blank">Conference on Robot Learning (CoRL) 2022</a></b>
				<br>
				<font color="firebrick"><b>Spotlight</b></font> at
				<b><a href="https://aligning-robot-human-representations.github.io/" target="_blank">Aligning Robot Representations with Humans Workshop at CoRL  2022</a></b>
				<br>
				<a href="https://proceedings.mlr.press/v205/zhang23a.html">paper</a> |
          		<a href="https://sites.google.com/view/dr-hrl">website</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2021-expand.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation</font></b><br>
				<a href="https://guansuns.github.io/" target="_blank">Lin Guan</a>, 
				<a href="https://famishedrover.github.io/" target="_blank">Mudit Verma</a>, 
				<a href="https://www.linkedin.com/in/suna-guo-680a96159/" target="_blank">Sihang Guo</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://rakaposhi.eas.asu.edu/" target="_blank">Subbarao Kambhampati</a> 
				<br>
				<b><a href="https://neurips.cc/Conferences/2021" target="_blank">Advances in Neural Information Processing Systems (NeurIPS) 2021</a></b>
				<br>
				<font color="firebrick"><b>Spotlight</b></font> 
				<br>
				<a href="https://arxiv.org/abs/2006.14804">paper</a> |
				<a href="https://github.com/GuanSuns/Simple-Human-in-the-Loop-ML-Interface">code</a> |
				<a href="https://papertalk.org/papertalks/37212">talk</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2021-guidance.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Recent Advances in Leveraging Human Guidance for Sequential Decision-Making tasks</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>*, 				
				<a href="https://www.cs.utexas.edu/~faraztrb/" target="_blank">Faraz Torabi</a>*, 
				<a href="https://www.cs.utexas.edu/people/faculty-researchers/garrett-warnell" target="_blank">Garrett Warnell</a>, 
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
				<br>
				<b><a href="https://link.springer.com/journal/10458" target="_blank">Autonomous Agents and Multi-Agent Systems (JAAMAS) 2021</a></b>
				<br>
				<font color="firebrick"><b>Journal Paper (Survey)</b></font> 
				<br>
				<a href="https://link.springer.com/article/10.1007/s10458-021-09514-w">link</a> | 
				<a href="https://arxiv.org/abs/2107.05825">paper</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2021-tics.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">The Hierarchical Evolution in Human Vision Modeling</font></b><br>
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>
				<br>
				<b><a href="https://onlinelibrary.wiley.com/journal/17568765" target="_blank">Topics in Cognitive Sciences 2021</a></b>
				<br>
				<font color="firebrick"><b>Journal Paper</b></font> 
				<br>
				<a href="https://onlinelibrary.wiley.com/doi/10.1111/tops.12527">link</a> | 
				<a href="https://ai.stanford.edu/~zharu/publications/2020_TiCS_hier.pdf">free to read</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2020-gamma.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Parallel Neural Processing with Gamma Frequency Latencies</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>
				<br>
				<b><a href="https://direct.mit.edu/neco" target="_blank">Neural Computation 2020</a></b>
				<br>
				<font color="firebrick"><b>Journal Paper</b></font> 
				<br>
				<a href="https://www.mitpressjournals.org/doi/full/10.1162/neco_a_01301">link</a> |
				<a href="https://ai.stanford.edu/~zharu/publications/neco_a_01301.pdf">free to read</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2020-ahead.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.linkedin.com/in/calen-walshe-ab708364/" target="_blank">Calen Walshe</a>, 
				<a href="https://www.linkedin.com/in/zhuodeliu/" target="_blank">Zhuode Liu</a>, 
				<a href="https://guansuns.github.io/" target="_blank">Lin Guan</a>, 
				<a href="https://www.linkedin.com/in/karl-muller-025b5757/" target="_blank">Karl Muller</a>, 
				<a href="https://www.linkedin.com/in/jake-whritner-phd-366142189/" target="_blank">Jake Whritner</a>, 
				<a href="https://lucinezhang.github.io/" target="_blank">Luxin Zhang</a>, 
				<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>
				<br>
				<b><a href="https://ijcai20.org/" target="_blank">AAAI Conference on Artificial Intelligence (AAAI) 2020</a></b>
				<br>
				<font color="firebrick"><b>Oral Presentation</b></font> at
				<b><a href="https://ijcai20.org/" target="_blank">AAAI Reinforcement Learning in Games Workshop 2020</a></b>
				<br>
				<a href="https://arxiv.org/abs/1903.06754">paper</a> | 
				<a href="https://zenodo.org/record/3451402#.XYPhzOtKhhE">data</a> |
				<a href="https://github.com/asaran/IL-CGL/tree/master/BC-CGL">code</a> |
				<a href="http://ai.stanford.edu/~zharu/publications/2020_AAAI_Dataset_poster.pdf">poster</a> |
				<a href="http://ai.stanford.edu/~zharu/publications/AAAI2020-RLG.pdf">talk</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/agil.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">AGIL: Learning Attention from Human for Visuomotor Tasks</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.linkedin.com/in/zhuodeliu/" target="_blank">Zhuode Liu</a>, 
				<a href="https://lucinezhang.github.io/" target="_blank">Luxin Zhang</a>, 
				<a href="https://www.linkedin.com/in/jake-whritner-phd-366142189/" target="_blank">Jake Whritner</a>, 
				<a href="https://www.linkedin.com/in/karl-muller-025b5757/" target="_blank">Karl Muller</a>, 
				<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>
				<br>
				<b><a href="https://eccv2018.org/" target="_blank">European Conference on Computer Vision (ECCV) 2018</a></b>
				<br>
				<a href="https://arxiv.org/abs/1806.03960">paper</a> |
				<a href="https://zenodo.org/record/3451402#.XYPhzOtKhhE">data</a> |
				<a href="https://github.com/asaran/IL-CGL/tree/master/BC-CGL">code</a>
				<br>
				<a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/5090/4963">AAAI 2019 Doctoral Consortium</a> |
				<a href="https://ojs.aaai.org/index.php/AAAI/article/view/12147">AAAI 2018 Student Abstract</a> |
				<a href="http://ai.stanford.edu/~zharu/publications/2017_NIPS_AGIL.pdf">NIPS 2017 CIAI workshop </a>
				<br>
				<a href="https://jov.arvojournals.org/article.aspx?articleid=2699524">VSS 2018 abstract</a> |
				<a href="https://ai.stanford.edu/~zharu/publications/VSS_2018_Modeling_Complex_Perception-Action_Choices.pdf">VSS 2018 talk</a> |
				<a href="https://ccn.societyconference.com/documents/1031/5928d74a68ed3f3c458a258b.pdf">CCN 2017 version</a>
			</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/mirl.mov" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Modeling Sensory-Motor Decisions in Natural Behavior</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://shunzh.github.io/" target="_blank">Shun Zhang</a>, 
				<a href="https://www.linkedin.com/in/matthew-tong/" target="_blank">Matthew Tong</a>, 
				<a href="https://yuchencui.cc/" target="_blank">Yuchen Cui</a>, 
				<a href="https://www.pip.tu-darmstadt.de/ag_pip/index.en.jsp" target="_blank">Constatin Rothkopf</a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>, 
				<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>
				<br>
				<b><a href="https://journals.plos.org/ploscompbiol/" target="_blank">PLoS Computational Biology 2018</a></b>
				<br>
				<font color="firebrick"><b>Journal Paper</b></font> 
				<br>
				<a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006518&rev=2">link</a> |
				<a href="https://journals.plos.org/ploscompbiol/article/file?rev=2&id=10.1371/journal.pcbi.1006518&type=printable">paper</a> | 
				<a href="https://jov.arvojournals.org/article.aspx?articleid=2652133">VSS 2017 abstract</a> |
				<a href="https://ai.stanford.edu/~zharu/publications/2017_VSS_MIRL.pdf">VSS 2017 talk</a> |
				<a href="  http://ai.stanford.edu/~zharu/publications/2016_NETI_mirl.pdf">NETI 2016 poster</a>
			</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/robocup.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">UT Austin Villa: Project-Driven Research in AI and Robotics</font></b><br>
				<a href="https://www.cs.utexas.edu/~katie/index.html" target="_blank">Katie Genter</a>, 
				<a href="https://www.linkedin.com/in/patrick-macalpine-0ba700a/" target="_blank">Patrick MacAlpine</a>, 
				<a href="https://www.linkedin.com/in/jacob-menashe-10898b83/" target="_blank">Jacob Menashe</a>, 
				<a href="https://pages.cs.wisc.edu/~jphanna/" target="_blank">Josiah Hanna</a>, 
				<a href="https://eladlieb.weebly.com/" target="_blank">Elad Liebman</a>, 
				<a href="https://www.cs.utexas.edu/~sanmit/" target="_blank">Sanmit Narvekar</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
				<br>
				<b><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9670" target="_blank">IEEE Intelligent Systems 31(2) 2016</a></b>
				<br>
				<font color="firebrick"><b>Journal Paper</b></font> 
				<br>
				<a href="http://ieeexplore.ieee.org/document/7435178/?arnumber=7435178">link</a> |
				<a href="https://spl.robocup.org/open-source/">code</a> |
				<a href="https://www.cs.utexas.edu/~AustinVilla/?p=nao"> UT Austin Villa</a>
			</div>
			</div><hr>

        </script> 
        </div> 













		<script id="pubs_by_date" language="text">
            <font color="black">(*† indicates equal contribution)</font><br><hr>

			<div class="row">
				<div class="col-md-3">
					<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/uad.mp4" type="video/mp4">
						</video>
					</div>
				<div class="col-md-9">
				<b><font color="black">UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation</font></b><br>
				<a href="https://tangyihe.com/" target="_blank">Yihe Tang</a>, 
				<a href="https://wenlong.page/" target="_blank">Wenlong Huang</a>,
				<a href="https://www.wykac.com/" target="_blank">Yingke Wang</a>,
				<a href="https://www.chengshuli.me/" target="_blank">Chengshu Li</a>,
				<a href="https://www.linkedin.com/in/ryuan19/" target="_blank">Roy Yuan</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="https://2025.ieee-icra.org/" target="_blank">International Conference on Robotics and Automation (ICRA), 2025</a></b>
				<br>
				<font color="firebrick"><b>Best Paper Award Finalist</b></font> 
				<br>
				<font color="firebrick"><b>Best Paper Award Finalist in Robot Perception</b></font> 
				<br>
				<a href="https://x.com/RuohanZhang76/status/1924489528007426119">tl;dr</a> |
				<a href="https://arxiv.org/abs/2506.09284">paper</a> |
				<a href="https://unsup-affordance.github.io/">website</a> |
				<a href="https://github.com/TangYihe/unsup-affordance">code</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/brs.mp4" type="video/mp4">
						</video>
					</div>
				<div class="col-md-9">
				<b><font color="black">BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities</font></b><br>
				<a href="https://yunfanj.com/" target="_blank">Yunfan Jiang</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="https://jdw.ong/" target="_blank">Josiah Wong</a>,  
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>,
				<a href="https://yanjieze.com/" target="_blank">Yanjie Ze</a>,
				<a href="https://www.linkedin.com/in/hangyin0226/" target="_blank">Hang Yin</a>,
				<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>, 
				<a href="https://shurans.github.io/" target="_blank">Shuran Song</a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="" target="_blank">arxiv 2025</a></b>
				<br>
				<a href="https://x.com/RuohanZhang76/status/1900328330831991050">tl;dr</a> |
				<a href="https://arxiv.org/abs/2503.05652">paper</a> |
				<a href="https://behavior-robot-suite.github.io/">website</a> |
				<a href="https://behavior-robot-suite.github.io/docs/">documentation</a> |
				<a href="https://github.com/behavior-robot-suite/brs-ctrl">robot code</a> |
				<a href="https://github.com/behavior-robot-suite/brs-algo">algorithm code</a> 
				</div>
			</div><hr>


			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2025-shapecraft.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
					</div>
				<div class="col-md-9">
				<b><font color="black">ShapeCraft: Body-Aware and Semantics-Aware 3D Object Design</font></b><br>
				<a href="" target="_blank">Michelle Guo*</a>,
				<a href="https://www.mia-tang.com/" target="_blank">Mia Tang*</a>,
				<a href="https://www.hannahcha.com/" target="_blank">Hannah Cha</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="https://tml.stanford.edu/people/karen-liu" target="_blank">C. Karen Liu</a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><a href="https://wacv2025.thecvf.com/" target="_blank">IEEE/CVF Winter Conference (WACV) 2025</a></b>
				<br>
				<a href="https://www.youtube.com/watch?v=b2vo96IjzpY">tl;dr</a> |
				<a href="https://arxiv.org/abs/2412.03889">paper</a> |
				<a href="https://miatang13.github.io/Shape-Craft/">website</a> |
				<a href="">code</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2024-eai.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
					</div>
				<div class="col-md-9">
				<b><font color="black">Embodied Agent Interface: A Single Line to Evaluate LLMs for Embodied Decision Making</font></b><br>
				<a href="https://limanling.github.io/">Manling Li</a>*, 
				Shiyu Zhao*, 
				<a href="https://qinengwang-aiden.github.io/">Qineng Wang</a>*, 
				<a href="https://jameskrw.github.io/">Kangrui Wang</a>*, 
				<a href="https://bryanzhou008.github.io/">Yu Zhou</a>*, 
				<a href="https://www.linkedin.com/in/sanjana-srivastava5/" target="_blank">Sanjana Srivastava</a>, 
				<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>, 
				<a href="https://www.linkedin.com/in/tonyhlee/">Tony Lee</a>, 
				<a href="https://www.cs.columbia.edu/~lierranli/">Li Erran Li</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="http://weiyuliu.com/" target="_blank">Weiyu Liu</a>, 
				<a href="https://cs.stanford.edu/~pliang/">Percy Liang</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>,
				<a href="https://jiayuanm.com/" target="_blank">Jiayuan Mao</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><a href="https://neurips.cc/" target="_blank">NeurIPS Datasets and Benchmarks Track 2024</a></b>
				<br>
				<font color="firebrick"><b>Oral Presentation</b></font> 
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1855401520793264302">tl;dr</a> |
				<a href="https://arxiv.org/abs/2410.07166">paper</a> |
				<a href="https://embodied-agent-interface.github.io/">website</a> |
				<a href="https://github.com/embodied-agent-interface/embodied-agent-interface">code</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2024-marple2.png" style="border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">MARPLE: A Benchmark for Long-Horizon Inference</font></b><br>
				<a href="https://www.linkedin.com/in/emily-jin-020/" target="_blank">Emily Jin</a>*, 
				<a href="https://www.linkedin.com/in/zhuoyi-huang/" target="_blank">Zhuoyi Huang</a>*, 
				<a href="https://janphilippfranken.github.io/">Jan-Philipp Fränken</a>,
                <a href="http://weiyuliu.com/">Weiyu Liu</a>,
                <a href="https://www.hannahcha.com/about">Hannah Cha</a>,
				<a href="https://sarahawu.github.io/">Sarah A. Wu</a>,
                <a href="https://www.erikbrockbank.com/">Erik Brockbank</a>,
                <a href="https://ai.stanford.edu/~zharu/"><u>Ruohan Zhang</u></a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
                <a href="https://cicl.stanford.edu/member/tobias_gerstenberg/">Tobias Gerstenberg</a>
				<br>
				<b><a href="https://neurips.cc/" target="_blank">NeurIPS Datasets and Benchmarks Track 2024</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1842333564467302559">tl;dr</a> |
				<a href="https://arxiv.org/abs/2410.01926">paper</a> |
				<a href="https://marple-benchmark.github.io/">website</a> |
				<a href="https://github.com/marple-benchmark/marple">code</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/rekep.mp4" type="video/mp4">
						</video>
					</div>
				<div class="col-md-9">
				<b><font color="black">ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation</font></b><br>
				<a href="https://wenlong.page/" target="_blank">Wenlong Huang</a>,
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>*,
				<a href="https://yunzhuli.github.io/">Yunzhu Li</a>*,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="https://www.corl.org/" target="_blank">Conference on Robot Learning (CoRL) 2024</a></b>
				<br>
				<font color="firebrick"><b>Best Paper Award</b></font> at
				<b><a href="https://leap-workshop.github.io/" target="_blank">Learning Effective Abstractions for Planning (LEAP) Workshop at CoRL 2024</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1830534553267782005">tl;dr</a> |
				<a href="https://arxiv.org/abs/2409.01652">paper</a> |
				<a href="https://rekep-robot.github.io/">website</a> |
				<a href="https://github.com/huangwl18/ReKep">code</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/transic.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction</font></b><br>
				<a href="https://yunfanj.com/" target="_blank">Yunfan Jiang</a>, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="https://www.corl.org/" target="_blank">Conference on Robot Learning (CoRL) 2024</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1791603579159269835">tl;dr</a> |
				<a href="https://arxiv.org/abs/2405.10315">paper</a> |
				<a href="https://transic-robot.github.io/">website</a> |
				<a href="https://github.com/transic-robot/transic">code</a> 
				</div>
			</div><hr>


			<div class="row">
				<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/acdc.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Automated Creation of Digital Cousins for Robust Policy Learning</font></b><br>
				<a href="https://rogerdai1217.github.io/" target="_blank">Tianyuan Dai</a>,  
				<a href="https://jdw.ong/" target="_blank">Josiah Wong</a>,  
				<a href="https://yunfanj.com/" target="_blank">Yunfan Jiang</a>, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="https://www.corl.org/" target="_blank">Conference on Robot Learning (CoRL) 2024</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1844448510864916511">tl;dr</a> |
				<a href="https://arxiv.org/abs/2410.07408">paper</a> |
				<a href="https://digital-cousins.github.io/">website</a> |
				<a href="https://github.com/cremebrule/digital-cousins">code</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="images/2024-blade.png" style="border:0px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Learning Compositional Behaviors from Demonstration and Language</font></b><br>
				<a href="http://weiyuliu.com/" target="_blank">Weiyu Liu*</a>, 
				<a href="https://neilnie.com/" target="_blank">Neil Nie*</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://jiayuanm.com/" target="_blank">Jiayuan Mao†</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu†</a>
				<br>
				<b><a href="https://www.corl.org/" target="_blank">Conference on Robot Learning (CoRL) 2024</a></b>
				<br>
				<font color="firebrick"><b>Oral Presentation</b></font> at 
				<b><a href="https://leap-workshop.github.io/" target="_blank">Workshop on Learning Effective Abstractions for Planning (LEAP) at CoRL 2024</a></b>
				<br>
				<a href="https://x.com/Weiyu_Liu_/status/1854508231093092360">tl;dr</a> |
				<a href="https://blade-bot.github.io/blade.pdf">paper</a> |
				<a href="https://blade-bot.github.io/">website</a> |
				<a href="">code</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/openx.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</font></b><br>
				Open X-Embodiment Collaboration
				<br>
				<b><a href="https://2024.ieee-icra.org/" target="_blank">International Conference on Robotics & Automation (ICRA) 2024</a></b>
				<br>
				<font color="firebrick"><b>Best Paper Award</b></font> 
				<br>
				<a href="https://twitter.com/QuanVng/status/1709209020341669988">tl;dr</a> |
        		<a href="https://arxiv.org/abs/2310.08864">paper</a> |
       			<a href="https://robotics-transformer-x.github.io/">website</a> |
       			<a href="https://github.com/google-deepmind/open_x_embodiment">code</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/dexcap.mp4" type="video/mp4">
					</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation</font></b><br>
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a>, 
				<a href="https://www.linkedin.com/in/weizhuo-ken-wang-ba4921119/" target="_blank">Weizhuo Wang</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://tml.stanford.edu/people/karen-liu" target="_blank">C. Karen Liu</a>
				<br>
				<b><a href="https://roboticsconference.org/" target="_blank">Robotics: Science and Systems (RSS) 2024</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1769533513940738127">tl;dr</a> |
				<a href="https://arxiv.org/abs/2403.07788">paper</a> |
				<a href="https://dex-cap.github.io/">website</a> |
				<a href="https://docs.google.com/document/d/1ANxSA_PctkqFf3xqAkyktgBgDWEbrFK7b1OnJe54ltw/edit#heading=h.yxlxo67jgfyx">hardware</a> |
				<a href="https://github.com/j96w/DexCap">code</a> 
				</div>
			</div><hr>


			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2024-marple.jpg" style="border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Whodunnit? Inferring What Happened from Multimodal Evidence</font></b><br>
				<a href="https://sarahawu.github.io/">Sarah A. Wu</a>*,
                <a href="https://www.erikbrockbank.com/">Erik Brockbank</a>*,
                <a href="https://www.hannahcha.com/about">Hannah Cha</a>,
                <a href="https://janphilippfranken.github.io/">Jan-Philipp Fränken</a>,
				<a href="https://www.linkedin.com/in/emily-jin-020/" target="_blank">Emily Jin</a>, 
				<a href="https://www.linkedin.com/in/zhuoyi-huang/" target="_blank">Zhuoyi Huang</a>, 
                <a href="http://weiyuliu.com/">Weiyu Liu</a>,
                <a href="https://ai.stanford.edu/~zharu/"><u>Ruohan Zhang</u></a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
                <a href="https://cicl.stanford.edu/member/tobias_gerstenberg/">Tobias Gerstenberg</a>
				<br>
				<b><a href="https://cognitivesciencesociety.org/cogsci-2024/" target="_blank">Proceedings of the Annual Meeting of the Cognitive Science Society (CogSci) 2024</a></b>
				<br>
				<a href="https://twitter.com/tobigerstenberg/status/1816363909588115515">tl;dr</a> |
				<a href="https://escholarship.org/uc/item/1nq5p6m8">paper</a> |
				<a href="https://github.com/cicl-stanford/whodunnit_multimodal_inference">code</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-left: 30pt; width: 70%; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/bvs.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation</font></b><br>
				<a href="https://gyhandy.github.io/" target="_blank">Yunhao Ge</a>*, 
				<a href="https://tangyihe.com/" target="_blank">Yihe Tang</a>*, 
				<a href="https://cnut1648.github.io/" target="_blank">Jiashu Xu</a>*, 
				<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>*, 
				<a href="https://www.chengshuli.me/" target="_blank">Chengshu Li</a>,
				<a href="https://wensi-ai.github.io/" target="_blank">Wensi Ai</a>,
				<a href="https://web.stanford.edu/~benjm/" target="_blank">Benjamin Jose Martinez</a>,
				<a href="https://www.linkedin.com/in/arman-aydin/" target="_blank">Arman Aydin</a>,
				<a href="https://www.linkedin.com/in/mona-anvari/" target="_blank">Mona Anvari</a>,
				<a href="https://www.linkedin.com/in/ayush-chakravarthy/" target="_blank">Ayush K Chakravarthy</a>,
				<a href="https://kovenyu.com/" target="_blank">Hong-Xing Yu</a>,
				<a href="https://jdw.ong/" target="_blank">Josiah Wong</a>, 
				<a href="https://www.linkedin.com/in/sanjana-srivastava5/" target="_blank">Sanjana Srivastava</a>, 
				<a href="https://knight-hennessy.stanford.edu/people/sharon-lee" target="_blank">Sharon Lee</a>, 
				<a href="https://www.linkedin.com/in/shengxin-cindy-z-4080b525/" target="_blank">Shengxin Zha</a>, 
				<a href="http://ilab.usc.edu/itti/" target="_blank">Laurent Itti</a>, 
				<a href="https://yunzhuli.github.io/">Yunzhu Li</a>, 
				<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>, 
				<a href="https://aptx4869lm.github.io/">Miao Liu</a>, 
				<a href="https://pzzhang.github.io/pzzhang/">Pengchuan Zhang</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a> 
				<br>
				<b><a href="https://2024.ieee-icra.org/" target="_blank">Conference on Computer Vision and Pattern Recognition (CVPR) 2024</a></b>
				<br>
				<font color="firebrick"><b>Highlight</b></font> 
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1802470520493146214">tl;dr</a> |
          		<a href="https://arxiv.org/abs/2405.09546">paper</a> |
          		<a href="https://behavior-vision-suite.github.io/">website</a> |
          		<a href="https://github.com/behavior-vision-suite/behavior-vision-suite.github.io">code</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/telemoma.m4v" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation</font></b><br>
				<a href="https://shivindass.github.io/" target="_blank">Shivin Dass</a>, 
				<a href="https://wensi-ai.github.io/" target="_blank">Wensi Ai</a>, 
				<a href="https://yuqianjiang.us/" target="_blank">Yuqian Jiang</a>, 
				<a href="https://www.linkedin.com/in/imsamik/" target="_blank">Samik Singh</a>, 
				<a href="https://jiahenghu.github.io/" target="_blank">Jiaheng Hu</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>, 
				<a href="https://babbatem.github.io/" target="_blank">Ben Abbatematteo</a>, 
				<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>
				<br>
				<b><a href="" target="_blank">arxiv 2024</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1769535561998462982">tl;dr</a> |
				<a href="https://arxiv.org/abs/2403.07869">paper</a> |
				<a href="https://robin-lab.cs.utexas.edu/telemoma-web/">website</a> |
				<a href="https://github.com/UT-Austin-RobIn/telemoma">code</a> 
				</div>
			</div><hr>		

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 10pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/noir.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">NOIR: Neural Signal Operated Intelligent Robots for Everyday Activities</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>*, 
				<a href="https://knight-hennessy.stanford.edu/people/sharon-lee" target="_blank">Sharon Lee</a>*, 
				<a href="https://mj-hwang.github.io/" target="_blank">Minjune Hwang</a>*,
				<a href="https://misoshiruseijin.github.io/" target="_blank">Ayano Hiranaka</a>*, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://wensi-ai.github.io/" target="_blank">Wensi Ai</a>, 
				<a href="https://www.linkedin.com/in/ryantjj/" target="_blank">Jin Jie Ryan Tan</a>,
				<a href="https://www.linkedin.com/in/shreya-gupta-08/" target="_blank">Shreya Gupta</a>, 
				<a href="https://yih301.github.io/" target="_blank">Yilun Hao</a>, 
				<a href="https://www.gabrael.io/" target="_blank">Gabrael Levine</a>, 
				<a href="https://ruohangao.github.io/" target="_blank">Ruohan Gao</a>, 
				<a href="https://psychology.stanford.edu/people/anthony-norcia" target="_blank">Anthony Norcia</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><a href="https://www.corl2023.org/" target="_blank">Conference on Robot Learning (CoRL) 2023</a></b>
				<br>
				<b>
				<font color="firebrick"><b>Oral Presentation</b></font> at
				<a href="https://yantianzha.github.io/crl.github.io/" target="_blank">Bridging the Gap between Cognitive Science and Robot Learning Workshop at CoRL 2023</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1720525179028406492">tl;dr</a> |
				<a href="https://arxiv.org/abs/2311.01454">paper</a> |
				<a href="https://noir-corl.github.io/">website</a> | 
				<a href="https://hai.stanford.edu/news/wearable-device-allows-humans-control-robots-brain-waves">Stanford HAI News</a> |
				<a href="https://w.mgtv.com/b/607797/20399189.html?t=videoshare&externalsource=v_play&tc=jXKKosRPSAN7&f=wxf&dc=9c199a33-461e-488e-bc06-3b252fe3e950">MangoTV interview (湖南卫视专访)</a> |
				<a href="https://www.deeplearning.ai/the-batch/issue-249/">The Batch</a> |
				<a href="https://braintitan.medium.com/noir-brain-powered-robots-for-daily-tasks-2cebd6ff6046">Medium</a>
				</div>
			</div><hr>
	
			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/mimicplay.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Mimicplay: Long-horizon Imitation Learning by Watching Human Play</font></b><br>
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://jimfan.me/" target="_blank">Linxi Fan</a>, 
				<a href="https://web.stanford.edu/~jksun/" target="_blank">Jiankai Sun</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://faculty.cc.gatech.edu/~danfei/">Danfei Xu</a>, 
				<a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu</a>, 
				<a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a> 
				<br>
				<b><a href="https://www.corl2023.org/" target="_blank">Conference on Robot Learning (CoRL) 2023</a></b>
				<br>
				<font color="firebrick"><b>Finalist - Best Paper/Best Student Paper Awards <br>
					Finalist - Best Systems Paper Award <br> Oral Presentation</b></font> 
				<br>
				<a href="https://twitter.com/chenwang_j/status/1628792565385564160">tl;dr</a> |
				<a href="https://arxiv.org/abs/2302.12422">paper</a> |
				<a href="https://mimic-play.github.io/">website</a> |
				<a href="https://github.com/j96w/MimicPlay">code</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/voxposer.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models</font></b><br>
				<a href="https://wenlong.page/" target="_blank">Wenlong Huang</a>, <a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, <a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, <a href="https://yunzhuli.github.io/">Yunzhu Li</a>, <a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, <a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="https://www.corl2023.org/" target="_blank">Conference on Robot Learning (CoRL) 2023</a></b>
				<br>
				<font color="firebrick"><b>Oral Presentation</b></font> 
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1677376310811959296">tl;dr</a> |
				<a href="https://arxiv.org/abs/2307.05973">paper</a> |
				<a href="https://voxposer.github.io/">website</a> |
				<a href="https://github.com/huangwl18/VoxPoser">code</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2023-minibh.png" style="border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI</font></b><br>
				<a href="https://www.linkedin.com/in/emily-jin-020/" target="_blank">Emily Jin</a>,  				
				<a href="https://jiahenghu.github.io/" target="_blank">Jiaheng Hu</a>, 
				<a href="https://www.linkedin.com/in/zhuoyi-huang/" target="_blank">Zhuoyi Huang</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>
				<br>
				<b><a href="https://sites.google.com/view/aloe2023/home" target="_blank">Agent Learning in Open-Endedness (ALOE) Workshop at NeurIPS 2023</a></b>
				<br>
				<b><a href="https://aair-lab.github.io/genplan23/" target="_blank">Generalization in Planning (GenPlan) Workshop at NeurIPS 2023</a></b>
				<br>
				<a href="https://twitter.com/emilyzjin/status/1735788569921282537">tl;dr</a> |
				<a href="https://arxiv.org/abs/2310.01824">paper</a> |
				<a href="https://github.com/StanfordVL/mini_behavior/">code</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2023-vivr.png" style="border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Quantifying the Effect of Visual Impairments on Daily Activities in Virtual, Interactive Environments</font></b><br>
				<a href="https://wensi-ai.github.io/" target="_blank">Wensi Ai</a>, 
				<a href="https://knight-hennessy.stanford.edu/people/sharon-lee" target="_blank">Sharon Lee</a>,  
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>
				<br>
				<b><a href="https://cognitivesciencesociety.org/cogsci-2023/" target="_blank">Proceedings of the Annual Meeting of the Cognitive Science Society (CogSci) 2023</a></b>
				<br>
				<a href="https://escholarship.org/content/qt2sj3r0n2/qt2sj3r0n2.pdf">paper</a> |
				<a href="https://sites.google.com/view/vi-vr/?authuser=1">website</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/seed.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Primitive Skill-based Robot Learning from Human Evaluative Feedback</font></b><br>
				<a href="https://misoshiruseijin.github.io/" target="_blank">Ayano Hiranaka</a>*, 
				<a href="https://mj-hwang.github.io/" target="_blank">Minjune Hwang</a>*, 
				<a href="https://knight-hennessy.stanford.edu/people/sharon-lee" target="_blank">Sharon Lee</a>, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>
				<br>
				<b><a href="https://ieee-iros.org/" target="_blank">International Conference on Intelligent Robots and Systems (IROS) 2023</a></b>
				<br>
				<a href="https://arxiv.org/abs/2307.15801">paper</a> |
      			<a href="https://seediros23.github.io/">website</a> |
      			<a href="https://github.com/mj-hwang/seed">code</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2023-finv.png" style="margin-top: 10pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Partial-View Object View Synthesis via Filtering Inversion</font></b><br>
				<a href="https://sunfanyun.com/" target="_blank">Fan-Yun Sun</a>,
				<a href="https://research.nvidia.com/person/jonathan-tremblay" target="_blank">Jonathan Tremblay</a>,
				<a href="https://www.cs.cornell.edu/~valts/" target="_blank">Valts Blukis</a>,
				<a href="https://kevin-thankyou-lin.github.io/" target="_blank">Kevin Lin</a>,
				<a href="https://faculty.cc.gatech.edu/~danfei/" target="_blank">Danfei Xu</a>,
				<a href="https://www.borisivanovic.com/" target="_blank">Boris Ivanovic</a>,
				<a href="https://karkus.tilda.ws/" target="_blank">Peter Karkus</a>,
				<a href="https://cecas.clemson.edu/~stb/" target="_blank">Stan Birchfield</a>,
				<a href="https://homes.cs.washington.edu/~fox/" target="_blank">Dieter Fox</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="https://yunzhuli.github.io/">Yunzhu Li</a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://research.nvidia.com/person/marco-pavone" target="_blank">Marco Pavone</a>,
				<a href="https://ed.stanford.edu/faculty/nhaber" target="_blank">Nick Haber</a>
				<br>
				<b><a href="https://3dvconf.github.io/2024/" target="_blank">International Conference on 3D Vision (3DV) 2024</a></b>
				<br>
				<font color="firebrick"><b>Spotlight</b></font> 
				<br>
				<a href="https://arxiv.org/abs/2304.00673">paper</a> |
       			<a href="https://cs.stanford.edu/~sunfanyun/finv/">website</a> |
        		<a href="https://github.com/sunfanyunn/FINV">code</a>
				</div>
			</div><hr>


			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2023-sgm.png" style="margin-top: 10pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Modeling Dynamic Environments with Scene Graph Memory</font></b><br>
				<a href="https://www.andreykurenkov.com/">Andrey Kurenkov</a>, 
				<a href="https://mlingelbach.com/" target="_blank">Michael Lingelbach</a>,
				<a href="https://tanmay-agarwal.com/" target="_blank">Tanmay Agarwal</a>, 
				<a href="https://www.chengshuli.me/" target="_blank">Chengshu Li</a>, 
				<a href="https://www.linkedin.com/in/emily-jin-020/" target="_blank">Emily Jin</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
				<a href="https://www.linkedin.com/in/silvio-savarese-97b76114/">Silvio Savarese</a>, 
				<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>
				<br>
				<b><a href="https://icml.cc/Conferences/2023" target="_blank">International Conference on Machine Learning (ICML) 2023</a></b>
				<br>
				<a href="https://twitter.com/andrey_kurenkov/status/1664742905062338560">tl;dr</a> |
				<a href="https://arxiv.org/abs/2305.17537">paper</a> |
				<a href="https://github.com/andreykurenkov/modeling_env_dynamics">code</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2023-sgs.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Task-Driven Graph Attention for Hierarchical Relational Object Navigation</font></b><br>
				<a href="https://mlingelbach.com/" target="_blank">Michael Lingelbach</a>, 
				<a href="https://www.chengshuli.me/" target="_blank">Chengshu Li</a>, 
				<a href="https://mj-hwang.github.io/" target="_blank">Minjune Hwang</a>,
				<a href="https://www.andreykurenkov.com/">Andrey Kurenkov</a>, 
				<a href="https://www.linkedin.com/in/alanlou/">Alan Lou</a>,
				<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><a href="https://www.icra2023.org/" target="_blank">International Conference on Robotics and Automation (ICRA) 2023</a></b>
				<br>
				<a href="https://arxiv.org/abs/2306.13760">paper</a> |
				<a href="https://github.com/mjlbach/ssg">code</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 40pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/behavior.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation</font></b><br>
				<a href="https://www.chengshuli.me/" target="_blank">Chengshu Li</a>*, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>*, 
				<a href="https://jdw.ong/" target="_blank">Josiah Wong</a>*, 
				<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>*, 
				<a href="https://www.linkedin.com/in/sanjana-srivastava5/" target="_blank">Sanjana Srivastava</a>*, 
				<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>*, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>*, 
				<a href="https://www.gabrael.io/" target="_blank">Gabrael Levine</a>*, 
				<a href="https://wensi-ai.github.io/" target="_blank">Wensi Ai</a>*, 
				<a href="https://web.stanford.edu/~benjm/" target="_blank">Benjamin Jose Martinez</a>,
				<a href="https://www.linkedin.com/in/hangyin0226/" target="_blank">Hang Yin</a>,
				<a href="https://mlingelbach.com/" target="_blank">Michael Lingelbach</a>,
				<a href="https://mj-hwang.github.io/" target="_blank">Minjune Hwang</a>,
				<a href="https://misoshiruseijin.github.io/" target="_blank">Ayano Hiranaka</a>, 
				<a href="https://sujaygarlanka.com/" target="_blank">Sujay Garlanka</a>, 
				<a href="https://www.linkedin.com/in/arman-aydin/" target="_blank">Arman Aydin</a>, 
				<a href="https://knight-hennessy.stanford.edu/people/sharon-lee" target="_blank">Sharon Lee</a>, 
				<a href="https://web.stanford.edu/~jksun/" target="_blank">Jiankai Sun</a>,
				<a href="https://www.linkedin.com/in/mona-anvari/" target="_blank">Mona Anvari</a>,
				<a href="https://manasi-sharma.github.io/" target="_blank">Manasi Sharma</a>,
				<a href="https://www.linkedin.com/in/dhruvabansal2k/" target="_blank">Dhruva Bansal</a>,
				<a href="" target="_blank">Samuel Hunter</a>,
				<a href="https://kykim0.github.io/">Kyu-Young Kim</a>, 
				<a href="https://www.linkedin.com/in/alanlou/">Alan Lou</a>, 
				<a href="https://www.linkedin.com/in/caleb-matthews/">Caleb R Matthews</a>, 
				<a href="https://ivillar.github.io/">Ivan Villa-Renteria</a>, 
				<a href="https://www.linkedin.com/in/jerry-tang-b37026162/">Jerry Huayang Tang</a>, 
				<a href="">Claire Tang</a>, 
				<a href="https://fxia22.github.io/">Fei Xia</a>, 
				<a href="https://yunzhuli.github.io/">Yunzhu Li</a>, 
				<a href="https://www.linkedin.com/in/silvio-savarese-97b76114/">Silvio Savarese</a>, 
				<a href="https://web.stanford.edu/~hyo/Home.html">Hyowon Gweon</a>, 
				<a href="https://tml.stanford.edu/people/karen-liu" target="_blank">C. Karen Liu</a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>     
				<br>
				<b><a href="https://corl2022.org/" target="_blank">Conference on Robot Learning (CoRL) 2022</a></b>
				<br>
				<font color="firebrick"><b>Best Paper Nomination</b></font> 
				<br>
				<font color="firebrick"><b>Oral Presentation</b></font> 
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1771019123298066812">tl;dr</a> |
				<a href="https://arxiv.org/abs/2403.09227">paper</a> |
				<a href="https://behavior.stanford.edu/">website</a> |
				<a href="https://github.com/StanfordVL/OmniGibson">code</a> |
				<a href="https://behavior.stanford.edu/omnigibson/getting_started/installation.html">guide</a> |
				<a href="https://discord.com/invite/bccR5vGFEx">support</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/dr-hrl.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">A Dual Representation Framework for Robot Learning with Human Guidance</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>*, 
				<a href="https://www.linkedin.com/in/dhruvabansal2k/" target="_blank">Dhruva Bansal</a>*, 
				<a href="https://yih301.github.io/" target="_blank">Yilun Hao</a>*, 
				<a href="https://misoshiruseijin.github.io/" target="_blank">Ayano Hiranaka</a>, 
				<a href="https://gaojl19.github.io/">Jialu Gao</a>, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><a href="https://corl2022.org/" target="_blank">Conference on Robot Learning (CoRL) 2022</a></b>
				<br>
				<font color="firebrick"><b>Spotlight</b></font> at
				<b><a href="https://aligning-robot-human-representations.github.io/" target="_blank">Aligning Robot Representations with Humans Workshop at CoRL  2022</a></b>
				<br>
				<a href="https://proceedings.mlr.press/v205/zhang23a.html">paper</a> |
          		<a href="https://sites.google.com/view/dr-hrl">website</a>
				</div>
			</div><hr>


			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2022-imma.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Interaction Modeling with Multiplex Attention</font></b><br>
				<a href="https://sunfanyun.com/">Fan-Yun Sun</a>, 
				<a href="https://ikauvar.github.io/">Isaac Kauvar</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://jiachenli94.github.io/">Jiachen Li</a>, 
				<a href="https://mykel.kochenderfer.com/">Mykel J. Kochenderfer</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
				<a href="https://www.autonomousagents.stanford.edu/">Nick Haber</a>
				<br>
				<b><a href="https://neurips.cc/Conferences/2022" target="_blank">Advances in Neural Information Processing Systems (NeurIPS) 2022</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1579529563616342016">tl;dr</a> |
				<a href="https://arxiv.org/abs/2208.10660">paper</a> |
				<a href="https://sites.google.com/view/dr-hrl">website</a> |
				<a href="https://github.com/fanyun-sun/IMMA">code</a>
				</div>
			</div><hr>


			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2022-gradient.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">How to Train your Decision-Making AIs?</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.linkedin.com/in/dhruvabansal2k/" target="_blank">Dhruva Bansal</a>
				<br>
				<b><a href="https://thegradient.pub/" target="_blank">The Gradient 2022</a></b>
				<br>
				<a href="https://thegradient.pub/how-to-train-your-decision-making-ais/">link</a> | 
				<a href="https://ai.stanford.edu/~zharu/publications/2022_TheGradient_HumanTrainAI.pdf">free to read</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2022-attention.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Selective Visual Attention during Public Speaking in an Immersive Context</font></b><br>
				<a href="https://www.paloaltou.edu/faculty/mikael-rubin-phd" target="_blank">Mikael Rubin</a>, 
				<a href="https://www.linkedin.com/in/suna-guo-680a96159/" target="_blank">Sihang Guo</a>, 
				<a href="https://www.linkedin.com/in/karl-muller-025b5757/" target="_blank">Karl Muller</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://labs.la.utexas.edu/telch/" target="_blank">Michael Telch</a>,
				<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>
				<br>
				<b><a href="https://link.springer.com/journal/13414" target="_blank">Attention, Perception, & Psychophysics 2022</a></b>
				<br>
				<font color="firebrick"><b>Journal Paper</b></font> 
				<br>
				<a href="https://link.springer.com/article/10.3758/s13414-021-02430-x">link</a> | 
        		<a href="https://link.springer.com/content/pdf/10.3758/s13414-021-02430-x.pdf">paper</a> |
				<a href="https://osf.io/zg9xh/">data</a> |
       			<a href="https://jov.arvojournals.org/article.aspx?articleid=2777608">VSS 2021 abstract</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/mvha.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Machine versus Human Attention in Deep Reinforcement Learning Tasks</font></b><br>
				<a href="https://www.linkedin.com/in/suna-guo-680a96159/" target="_blank">Sihang Guo</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://cranial-xix.github.io/" target="_blank">Bo Liu</a>, 
				<a href="https://zhuyifengzju.github.io/" target="_blank">Yifeng Zhu</a>, 
				<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>,
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>,
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
				<br>
				<b><a href="https://neurips.cc/Conferences/2021" target="_blank">Advances in Neural Information Processing Systems (NeurIPS) 2021</a></b>
				<br>
      			<a href="https://arxiv.org/abs/2010.15942">paper</a> |
				<a href="https://zenodo.org/records/3451402">data</a> |  
      			<a href="https://jov.arvojournals.org/article.aspx?articleid=2776905">VSS 2021 abstract</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2021-expand.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation</font></b><br>
				<a href="https://guansuns.github.io/" target="_blank">Lin Guan</a>, 
				<a href="https://famishedrover.github.io/" target="_blank">Mudit Verma</a>, 
				<a href="https://www.linkedin.com/in/suna-guo-680a96159/" target="_blank">Sihang Guo</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://rakaposhi.eas.asu.edu/" target="_blank">Subbarao Kambhampati</a> 
				<br>
				<b><a href="https://neurips.cc/Conferences/2021" target="_blank">Advances in Neural Information Processing Systems (NeurIPS) 2021</a></b>
				<br>
				<font color="firebrick"><b>Spotlight</b></font> 
				<br>
				<a href="https://arxiv.org/abs/2006.14804">paper</a> |
				<a href="https://github.com/GuanSuns/Simple-Human-in-the-Loop-ML-Interface">code</a> |
				<a href="https://papertalk.org/papertalks/37212">talk</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2021-guidance.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Recent Advances in Leveraging Human Guidance for Sequential Decision-Making tasks</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>*, 				
				<a href="https://www.cs.utexas.edu/~faraztrb/" target="_blank">Faraz Torabi</a>*, 
				<a href="https://www.cs.utexas.edu/people/faculty-researchers/garrett-warnell" target="_blank">Garrett Warnell</a>, 
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
				<br>
				<b><a href="https://link.springer.com/journal/10458" target="_blank">Autonomous Agents and Multi-Agent Systems (JAAMAS) 2021</a></b>
				<br>
				<font color="firebrick"><b>Journal Paper (Survey)</b></font> 
				<br>
				<a href="https://link.springer.com/article/10.1007/s10458-021-09514-w">link</a> | 
				<a href="https://arxiv.org/abs/2107.05825">paper</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/ut-logo2.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">A Modular Attention Hypothesis for Modeling Visuomotor Behaviors</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>
				<br>
				Committee: <a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>, 
				<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>, 
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>, 
				<a href="https://www.cs.utexas.edu/~huth/" target="_blank">Alexander Huth</a>, 
				<a href="https://www.mpg.de/12309370/biological-cybernetics-dayan" target="_blank">Peter Dayan</a>
				<br>
				<b><a href="https://www.utexas.edu/" target="_blank">The University of Texas at Austin Ph.D. Dissertation 2021</a></b>
				<br>
				<a href="https://repositories.lib.utexas.edu/bitstream/handle/2152/86946/ZHANG-DISSERTATION-2021.pdf?sequence=1">paper</a> | 
				<a href="https://ai.stanford.edu/~zharu/publications/disser_slides.pdf">slides</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2021-cgl.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Efficiently Guiding Imitation Learning Algorithms with Human Gaze</font></b><br>
				<a href="https://aihub.org/author/akankshasaran/" target="_blank">Akanksha Saran</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://eshort.github.io/" target="_blank">Elaine Schaertl Short</a>,
				<a href="https://people.cs.umass.edu/~sniekum/" target="_blank">Scott Niekum</a>
				<br>
				<b><a href="https://aamas2021.soton.ac.uk/" target="_blank">Autonomous Agents and Multi-Agent Systems (AAMAS) 2021</a></b>
				<br>
				<a href="https://arxiv.org/abs/2002.12500">paper</a> |
				<a href="https://github.com/asaran/IL-CGL">code</a> |
				<a href="https://drive.google.com/file/d/1bvfnkqFGF8O39yaF1QOcvhrOx2CpZyxX/view?usp=sharing">slides</a> |
				<a href="https://techxplore.com/news/2020-03-imitation-algorithms-human.html">Tech Xplore News</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2021-tics.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">The Hierarchical Evolution in Human Vision Modeling</font></b><br>
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>
				<br>
				<b><a href="https://onlinelibrary.wiley.com/journal/17568765" target="_blank">Topics in Cognitive Sciences 2021</a></b>
				<br>
				<font color="firebrick"><b>Journal Paper</b></font> 
				<br>
				<a href="https://onlinelibrary.wiley.com/doi/10.1111/tops.12527">link</a> | 
				<a href="https://ai.stanford.edu/~zharu/publications/2020_TiCS_hier.pdf">free to read</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2020-gaze-survey.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Human Gaze Assisted Artificial Intelligence: A Review</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://aihub.org/author/akankshasaran/" target="_blank">Akanksha Saran</a>, 
				<a href="https://cranial-xix.github.io/" target="_blank">Bo Liu</a>, 
				<a href="https://zhuyifengzju.github.io/" target="_blank">Yifeng Zhu</a>, 
				<a href="https://www.linkedin.com/in/suna-guo-680a96159/" target="_blank">Sihang Guo</a>, 
				<a href="https://people.cs.umass.edu/~sniekum/" target="_blank">Scott Niekum</a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>, 
				<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>
				<br>
				<b><a href="https://ijcai20.org/" target="_blank">International Joint Conference on Artificial Intelligence (IJCAI) Survey Track 2020</a></b>
				<br>
				<a href="https://www.ijcai.org/Proceedings/2020/0689.pdf">paper</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2020-gamma.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Parallel Neural Processing with Gamma Frequency Latencies</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>
				<br>
				<b><a href="https://direct.mit.edu/neco" target="_blank">Neural Computation 2020</a></b>
				<br>
				<font color="firebrick"><b>Journal Paper</b></font> 
				<br>
				<a href="https://www.mitpressjournals.org/doi/full/10.1162/neco_a_01301">link</a> |
				<a href="https://ai.stanford.edu/~zharu/publications/neco_a_01301.pdf">free to read</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2020-ahead.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.linkedin.com/in/calen-walshe-ab708364/" target="_blank">Calen Walshe</a>, 
				<a href="https://www.linkedin.com/in/zhuodeliu/" target="_blank">Zhuode Liu</a>, 
				<a href="https://guansuns.github.io/" target="_blank">Lin Guan</a>, 
				<a href="https://www.linkedin.com/in/karl-muller-025b5757/" target="_blank">Karl Muller</a>, 
				<a href="https://www.linkedin.com/in/jake-whritner-phd-366142189/" target="_blank">Jake Whritner</a>, 
				<a href="https://lucinezhang.github.io/" target="_blank">Luxin Zhang</a>, 
				<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>
				<br>
				<b><a href="https://ijcai20.org/" target="_blank">AAAI Conference on Artificial Intelligence (AAAI) 2020</a></b>
				<br>
				<font color="firebrick"><b>Oral Presentation</b></font> at
				<b><a href="https://ijcai20.org/" target="_blank">AAAI Reinforcement Learning in Games Workshop 2020</a></b>
				<br>
				<a href="https://arxiv.org/abs/1903.06754">paper</a> | 
				<a href="https://zenodo.org/record/3451402#.XYPhzOtKhhE">data</a> |
				<a href="https://github.com/asaran/IL-CGL/tree/master/BC-CGL">code</a> |
				<a href="http://ai.stanford.edu/~zharu/publications/2020_AAAI_Dataset_poster.pdf">poster</a> |
				<a href="http://ai.stanford.edu/~zharu/publications/AAAI2020-RLG.pdf">talk</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2020-gamma2.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Cortical Spikes use Analog Sparse Coding</font></b><br>
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://lucgentetlab.wordpress.com/ " target="_blank">Luc Gentet</a>
				<br>
				<b><a href="https://www.biorxiv.org/" target="_blank">bioRxiv 2020</a></b>
				<br>
				<a href="https://www.biorxiv.org/content/10.1101/2020.10.19.331389v1">paper</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2020-attrl.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">An Initial Attempt of Combining Visual Selective Attention with Deep Reinforcement Learning</font></b><br>
				<a href="https://www.linkedin.com/in/liuyuezhang/" target="_blank">Liu Yuezhang</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>
				<br>
				<b><a href="https://arxiv.org/" target="_blank">arxiv 2020</a></b>
				<br>
				<a href="https://arxiv.org/abs/1811.04407">paper</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2019-guidance.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Leveraging Human Guidance for Deep Reinforcement Learning Tasks</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~faraztrb/" target="_blank">Faraz Torabi</a>, 
				<a href="https://guansuns.github.io/" target="_blank">Lin Guan</a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>, 
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
				<br>
				<b><a href="https://www.ijcai19.org/" target="_blank">International Joint Conference on Artificial Intelligence (IJCAI) Survey Track 2019</a></b>
				<br>
				<a href="https://arxiv.org/abs/1909.09906">paper</a>
			</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/agil.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">AGIL: Learning Attention from Human for Visuomotor Tasks</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.linkedin.com/in/zhuodeliu/" target="_blank">Zhuode Liu</a>, 
				<a href="https://lucinezhang.github.io/" target="_blank">Luxin Zhang</a>, 
				<a href="https://www.linkedin.com/in/jake-whritner-phd-366142189/" target="_blank">Jake Whritner</a>, 
				<a href="https://www.linkedin.com/in/karl-muller-025b5757/" target="_blank">Karl Muller</a>, 
				<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>
				<br>
				<b><a href="https://eccv2018.org/" target="_blank">European Conference on Computer Vision (ECCV) 2018</a></b>
				<br>
				<a href="https://arxiv.org/abs/1806.03960">paper</a> |
				<a href="https://zenodo.org/record/3451402#.XYPhzOtKhhE">data</a> |
				<a href="https://github.com/asaran/IL-CGL/tree/master/BC-CGL">code</a>
				<br>
				<a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/5090/4963">AAAI 2019 Doctoral Consortium</a> |
				<a href="https://ojs.aaai.org/index.php/AAAI/article/view/12147">AAAI 2018 Student Abstract</a> |
				<a href="http://ai.stanford.edu/~zharu/publications/2017_NIPS_AGIL.pdf">NIPS 2017 CIAI workshop </a>
				<br>
				<a href="https://jov.arvojournals.org/article.aspx?articleid=2699524">VSS 2018 abstract</a> |
				<a href="https://ai.stanford.edu/~zharu/publications/VSS_2018_Modeling_Complex_Perception-Action_Choices.pdf">VSS 2018 talk</a> |
				<a href="https://ccn.societyconference.com/documents/1031/5928d74a68ed3f3c458a258b.pdf">CCN 2017 version</a>
			</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/mirl.mov" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Modeling Sensory-Motor Decisions in Natural Behavior</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://shunzh.github.io/" target="_blank">Shun Zhang</a>, 
				<a href="https://www.linkedin.com/in/matthew-tong/" target="_blank">Matthew Tong</a>, 
				<a href="https://yuchencui.cc/" target="_blank">Yuchen Cui</a>, 
				<a href="https://www.pip.tu-darmstadt.de/ag_pip/index.en.jsp" target="_blank">Constatin Rothkopf</a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>, 
				<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>
				<br>
				<b><a href="https://journals.plos.org/ploscompbiol/" target="_blank">PLoS Computational Biology 2018</a></b>
				<br>
				<font color="firebrick"><b>Journal Paper</b></font> 
				<br>
				<a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006518&rev=2">link</a> |
				<a href="https://journals.plos.org/ploscompbiol/article/file?rev=2&id=10.1371/journal.pcbi.1006518&type=printable">paper</a> | 
				<a href="https://jov.arvojournals.org/article.aspx?articleid=2652133">VSS 2017 abstract</a> |
				<a href="https://ai.stanford.edu/~zharu/publications/2017_VSS_MIRL.pdf">VSS 2017 talk</a> |
				<a href="  http://ai.stanford.edu/~zharu/publications/2016_NETI_mirl.pdf">NETI 2016 poster</a>
			</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2018-qest.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Model Checking For Safe Navigation Among Humans</font></b><br>
				<a href="https://sjunges.github.io/" target="_blank">Sebastian Junges</a>,
				<a href="https://nilsjansen.org/" target="_blank">Nils Jansen</a>,
				<a href="https://www-i2.informatik.rwth-aachen.de/~katoen/" target="_blank">Joost-Pieter Katoen</a>,
				<a href="https://www.ae.utexas.edu/people/faculty/faculty-directory/topcu" target="_blank">Ufuk Topcu</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>
				<br>
				<b><a href="https://www.qest.org/qest2018/" target="_blank">International Conference on Quantitative Evaluation of SysTem (QEST) 2018</a></b>
				<br>
				<a href="https://link.springer.com/chapter/10.1007/978-3-319-99154-2_13">link</a> |
				<a href="https://link.springer.com/content/pdf/10.1007/978-3-319-99154-2.pdf">paper</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2017-robocup-ball.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Fast and Precise Black and White Ball Detection for RoboCup Soccer</font></b><br>
				<a href="https://www.linkedin.com/in/jacob-menashe-10898b83/" target="_blank">Jacob Menashe</a>,
				<a href="http://joshkelle.com/" target="_blank">Josh Kelle</a>,
				<a href="https://www.cs.utexas.edu/~katie/index.html" target="_blank">Katie Genter</a>,
				<a href="https://pages.cs.wisc.edu/~jphanna/" target="_blank">Josiah Hanna</a>,
				<a href="https://eladlieb.weebly.com/" target="_blank">Elad Liebman</a>,
				<a href="https://www.cs.utexas.edu/~sanmit/" target="_blank">Sanmit Narvekar</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
				<br>
				<b><a href="https://2017.robocup.org/eng/symposium.html" target="_blank">RoboCup Symposium 2017</a></b>
				<br>
				<a href="https://2017.robocup.org/file/symposium/RoboCup_Symposium_2017_paper_20.pdf">paper</a> |
				<a href="https://spl.robocup.org/open-source/">code</a> |
				<a href="https://www.cs.utexas.edu/~AustinVilla/?p=nao"> UT Austin Villa</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2017-gdmm.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Greedy Direction Method of Multiplier for MAP Inference of Large Output Domain</font></b><br>
				<a href="http://wueh4.q-huan.link/english/dstd-en/dstd-gxy/dstd-gxy-rgznysjkx/202403/50277.html" target="_blank">Xiangru Huang</a>, 
				<a href="http://ianyen.site/" target="_blank">Ian E.H. Yen</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~huangqx/" target="_blank">Qixing Huang</a>, 
				<a href="https://www.cs.cmu.edu/~pradeepr/" target="_blank">Pradeep Ravikumar</a>, 
				<a href="https://www.linkedin.com/in/inderjit-dhillon-a20888b0/" target="_blank">Inderjit S. Dhillon</a> 
				<br>
				<b><a href="https://aistats.org/aistats2017/" target="_blank">Artificial Intelligence and Statistics (AISTATS) 2017</a></b>
				<br>
				<a href="http://proceedings.mlr.press/v54/huang17a">paper</a> |
				<a href="https://github.com/xiangruhuang/FastStructPred">code</a>
				</div>
			</div><hr>

			<!-- <div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2017-art.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Participatory Art Museum: Collecting and Modeling Crowd Opinions</font></b><br>
				<a href="https://www.linkedin.com/in/edith-zeng/" target="_blank">Xiaoyu Zeng</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>
				<br>
				<b><a href="https://aaai.org/conference/aaai/aaai17/" target="_blank">AAAI Conference on Artificial Intelligence (AAAI) Student Abstract 2017</a></b>
				<br>
				<a href="https://ojs.aaai.org/index.php/AAAI/article/view/11072">paper</a>
				</div>
			</div><hr> -->

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/robocup.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">UT Austin Villa: Project-Driven Research in AI and Robotics</font></b><br>
				<a href="https://www.cs.utexas.edu/~katie/index.html" target="_blank">Katie Genter</a>, 
				<a href="https://www.linkedin.com/in/patrick-macalpine-0ba700a/" target="_blank">Patrick MacAlpine</a>, 
				<a href="https://www.linkedin.com/in/jacob-menashe-10898b83/" target="_blank">Jacob Menashe</a>, 
				<a href="https://pages.cs.wisc.edu/~jphanna/" target="_blank">Josiah Hanna</a>, 
				<a href="https://eladlieb.weebly.com/" target="_blank">Elad Liebman</a>, 
				<a href="https://www.cs.utexas.edu/~sanmit/" target="_blank">Sanmit Narvekar</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
				<br>
				<b><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9670" target="_blank">IEEE Intelligent Systems 31(2) 2016</a></b>
				<br>
				<font color="firebrick"><b>Journal Paper</b></font> 
				<br>
				<a href="http://ieeexplore.ieee.org/document/7435178/?arnumber=7435178">link</a> |
				<a href="https://spl.robocup.org/open-source/">code</a> |
				<a href="https://www.cs.utexas.edu/~AustinVilla/?p=nao"> UT Austin Villa</a>
			</div>
			</div><hr>


			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2016-gdmm.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Dual Decomposed Learning with Factorwise Oracle for Structural SVM of Large Output Domain</font></b><br>
				<a href="http://ianyen.site/" target="_blank">Ian E.H. Yen</a>, 
				<a href="http://wueh4.q-huan.link/english/dstd-en/dstd-gxy/dstd-gxy-rgznysjkx/202403/50277.html" target="_blank">Xiangru Huang</a>, 
				<a href="https://www.linkedin.com/in/kai-zhong-29089436/" target="_blank">Kai Zhong</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.cmu.edu/~pradeepr/" target="_blank">Pradeep Ravikumar</a>, 
				<a href="https://www.linkedin.com/in/inderjit-dhillon-a20888b0/" target="_blank">Inderjit S. Dhillon</a> 
				<br>
				<b><a href="https://neurips.cc/Conferences/2016" target="_blank">Advances in Neural Information Processing Systems (NIPS) 2016</a></b>
				<br>
				<a href="https://proceedings.neurips.cc/paper_files/paper/2016/hash/7e83722522e8aeb7512b7075311316b7-Abstract.html">paper</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/aim.mp4" type="video/mp4">
					</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">Decision-Making Policies for Heterogeneous Autonomous Multi-Agent Systems with Safety Constraints</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://cse.umn.edu/aem/yue-yu" target="_blank">Yue Yu</a>,
				<a href="https://www.linkedin.com/in/mahmoud-el-chamie-3a9a6b25/" target="_blank">Mahmoud El Chamie</a>,
				<a href="https://uwacl.com/" target="_blank">Behçet Açikmese</a>,
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>
				<br>
				<b><a href="https://ijcai-16.org/" target="_blank">International Joint Conference on Artificial Intelligence (IJCAI) 2016</a></b>
				<br>
				<a href="http://www.ijcai.org/Proceedings/16/Papers/084.pdf">paper</a>
			</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2016-msy.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Maximum Sustainable Yield Problem for Robot Foraging and Construction System</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.youtube.com/@zhaosong2031" target="_blank">Zhao Song</a>				 
				<br>
				<b><a href="https://ijcai-16.org/" target="_blank">International Joint Conference on Artificial Intelligence (IJCAI) 2016</a></b>
				<br>
				<a href="http://www.ijcai.org/Proceedings/16/Papers/387.pdf">paper</a>
			</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/austin-villa.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">UT Austin Villa 2017 Team Description Paper for the Standard Platform League</font></b><br>
				<a href="https://www.cs.utexas.edu/~katie/index.html" target="_blank">Katie Genter</a>, 				
				<a href="https://pages.cs.wisc.edu/~jphanna/" target="_blank">Josiah Hanna</a>,
				<a href="http://joshkelle.com/" target="_blank">Josh Kelle</a>, 
				<a href="https://eladlieb.weebly.com/" target="_blank">Elad Liebman</a>, 
				<a href="https://www.linkedin.com/in/jacob-menashe-10898b83/" target="_blank">Jacob Menashe</a>, 
				<a href="https://www.cs.utexas.edu/~sanmit/" target="_blank">Sanmit Narvekar</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
				<br>
				<b><a href="https://www.robocup2017.org/lander" target="_blank">RoboCup 2017</a></b>
				<br>
				<a href="https://2017.robocup.org/file/symposium/soccer_std_plf/UT_TDP17.pdf">paper</a> |
				<a href="https://www.cs.utexas.edu/~AustinVilla/?p=nao"> UT Austin Villa</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/austin-villa.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">UT Austin Villa 2016 Team Description Paper for the Standard Platform League</font></b><br>
				<a href="https://www.cs.utexas.edu/~katie/index.html" target="_blank">Katie Genter</a>, 				
				<a href="https://pages.cs.wisc.edu/~jphanna/" target="_blank">Josiah Hanna</a>,
				<a href="http://joshkelle.com/" target="_blank">Josh Kelle</a>, 
				<a href="https://eladlieb.weebly.com/" target="_blank">Elad Liebman</a>, 
				<a href="https://www.linkedin.com/in/jacob-menashe-10898b83/" target="_blank">Jacob Menashe</a>, 
				<a href="https://www.cs.utexas.edu/~sanmit/" target="_blank">Sanmit Narvekar</a>, 
				<a href="" target="_blank">Rishi Shah</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
				<br>
				<b><a href="https://2016.robocup.org/web/index-2.html" target="_blank">RoboCup 2016</a></b>
				<br>
				<a href="https://2016.robocup.org/web/images/robocup_2016_spl_tdp_utaustinvilla.pdf">paper</a> |
				<a href="https://www.cs.utexas.edu/~AustinVilla/?p=nao"> UT Austin Villa</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/austin-villa.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">UT Austin Villa 2015 Team Description Paper for the Standard Platform League</font></b><br>
				<a href="https://www.cs.utexas.edu/~katie/index.html" target="_blank">Katie Genter</a>, 
				<a href="https://pages.cs.wisc.edu/~jphanna/" target="_blank">Josiah Hanna</a>, 
				<a href="https://eladlieb.weebly.com/" target="_blank">Elad Liebman</a>, 
				<a href="https://www.linkedin.com/in/jacob-menashe-10898b83/" target="_blank">Jacob Menashe</a>,
				<a href="https://www.cs.utexas.edu/~sanmit/" target="_blank">Sanmit Narvekar</a>, 
				<a href="https://www.eecs.tufts.edu/~jsinapov/" target="_blank">Jivko Sinapov</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>      
				<br>
				<b><a href="https://www.robocup2015.org/" target="_blank">RoboCup 2015</a></b>
				<br>
				<a href="http://robocup2015.oss-cn-shenzhen.aliyuncs.com/TeamDescriptionPapers/StandardPlatform/RoboCup_Symposium_2015_submission_72.pdf">paper</a> |
				<a href="https://www.cs.utexas.edu/~AustinVilla/?p=nao"> UT Austin Villa</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2015-mrl.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Global Policy Construction in Modular Reinforcement Learning</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.youtube.com/@zhaosong2031" target="_blank">Zhao Song</a>,				 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>
				<br>
				<b><a href="https://aaai.org/conference/aaai/aaai15/" target="_blank">AAAI Conference on Artificial Intelligence (AAAI) Student Abstract 2015</a></b>
				<br>
				<a href="https://cdn.aaai.org/ojs/9736/9736-13-13264-1-2-20201228.pdf">paper</a>
				</div>
			</div><hr>

			</script> 












		<script id="pubs_by_topic" language="text">
		
		<font color="black">(*† indicates equal contribution)</font><br><hr>
		
		<div id="db" style="padding-top: 80px; margin-top: -80px;">
		  <h5>Datasets and Benchmarks</h5>
		</div><br>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2024-eai.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
					</div>
				<div class="col-md-9">
				<b><font color="black">Embodied Agent Interface: A Single Line to Evaluate LLMs for Embodied Decision Making</font></b><br>
				<a href="https://limanling.github.io/">Manling Li</a>*, 
				Shiyu Zhao*, 
				<a href="https://qinengwang-aiden.github.io/">Qineng Wang</a>*, 
				<a href="https://jameskrw.github.io/">Kangrui Wang</a>*, 
				<a href="https://bryanzhou008.github.io/">Yu Zhou</a>*, 
				<a href="https://www.linkedin.com/in/sanjana-srivastava5/" target="_blank">Sanjana Srivastava</a>, 
				<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>, 
				<a href="https://www.linkedin.com/in/tonyhlee/">Tony Lee</a>, 
				<a href="https://www.cs.columbia.edu/~lierranli/">Li Erran Li</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="http://weiyuliu.com/" target="_blank">Weiyu Liu</a>, 
				<a href="https://cs.stanford.edu/~pliang/">Percy Liang</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>,
				<a href="https://jiayuanm.com/" target="_blank">Jiayuan Mao</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><a href="https://neurips.cc/" target="_blank">NeurIPS Datasets and Benchmarks Track 2024</a></b>
				<br>
				<font color="firebrick"><b>Oral Presentation</b></font> 
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1855401520793264302">tl;dr</a> |
				<a href="https://arxiv.org/abs/2410.07166">paper</a> |
				<a href="https://embodied-agent-interface.github.io/">website</a> |
				<a href="https://github.com/embodied-agent-interface/embodied-agent-interface">code</a> 
				</div>
			</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2024-marple2.png" style="border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">MARPLE: A Benchmark for Long-Horizon Inference</font></b><br>
				<a href="https://www.linkedin.com/in/emily-jin-020/" target="_blank">Emily Jin</a>*, 
				<a href="https://www.linkedin.com/in/zhuoyi-huang/" target="_blank">Zhuoyi Huang</a>*, 
				<a href="https://janphilippfranken.github.io/">Jan-Philipp Fränken</a>,
                <a href="http://weiyuliu.com/">Weiyu Liu</a>,
                <a href="https://www.hannahcha.com/about">Hannah Cha</a>,
				<a href="https://sarahawu.github.io/">Sarah A. Wu</a>,
                <a href="https://www.erikbrockbank.com/">Erik Brockbank</a>,
                <a href="https://ai.stanford.edu/~zharu/"><u>Ruohan Zhang</u></a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
                <a href="https://cicl.stanford.edu/member/tobias_gerstenberg/">Tobias Gerstenberg</a>
				<br>
				<b><a href="https://neurips.cc/" target="_blank">NeurIPS Datasets and Benchmarks Track 2024</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1842333564467302559">tl;dr</a> |
				<a href="https://arxiv.org/abs/2410.01926">paper</a> |
				<a href="https://marple-benchmark.github.io/">website</a> |
				<a href="https://github.com/marple-benchmark/marple">code</a>
				</div>
			</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/openx.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</font></b><br>
				Open X-Embodiment Collaboration
				<br>
				<b><a href="https://2024.ieee-icra.org/" target="_blank">International Conference on Robotics & Automation (ICRA) 2024</a></b>
				<br>
				<font color="firebrick"><b>Best Paper Award</b></font> 
				<br>
				<a href="https://twitter.com/QuanVng/status/1709209020341669988">tl;dr</a> |
        		<a href="https://arxiv.org/abs/2310.08864">paper</a> |
       			<a href="https://robotics-transformer-x.github.io/">website</a> |
       			<a href="https://github.com/google-deepmind/open_x_embodiment">code</a> 
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-left: 30pt; width: 70%; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/bvs.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation</font></b><br>
				<a href="https://gyhandy.github.io/" target="_blank">Yunhao Ge</a>*, 
				<a href="https://tangyihe.com/" target="_blank">Yihe Tang</a>*, 
				<a href="https://cnut1648.github.io/" target="_blank">Jiashu Xu</a>*, 
				<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>*, 
				<a href="https://www.chengshuli.me/" target="_blank">Chengshu Li</a>,
				<a href="https://wensi-ai.github.io/" target="_blank">Wensi Ai</a>,
				<a href="https://web.stanford.edu/~benjm/" target="_blank">Benjamin Jose Martinez</a>,
				<a href="https://www.linkedin.com/in/arman-aydin/" target="_blank">Arman Aydin</a>,
				<a href="https://www.linkedin.com/in/mona-anvari/" target="_blank">Mona Anvari</a>,
				<a href="https://www.linkedin.com/in/ayush-chakravarthy/" target="_blank">Ayush K Chakravarthy</a>,
				<a href="https://kovenyu.com/" target="_blank">Hong-Xing Yu</a>,
				<a href="https://jdw.ong/" target="_blank">Josiah Wong</a>, 
				<a href="https://www.linkedin.com/in/sanjana-srivastava5/" target="_blank">Sanjana Srivastava</a>, 
				<a href="https://knight-hennessy.stanford.edu/people/sharon-lee" target="_blank">Sharon Lee</a>, 
				<a href="https://www.linkedin.com/in/shengxin-cindy-z-4080b525/" target="_blank">Shengxin Zha</a>, 
				<a href="http://ilab.usc.edu/itti/" target="_blank">Laurent Itti</a>, 
				<a href="https://yunzhuli.github.io/">Yunzhu Li</a>, 
				<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>, 
				<a href="https://aptx4869lm.github.io/">Miao Liu</a>, 
				<a href="https://pzzhang.github.io/pzzhang/">Pengchuan Zhang</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a> 
				<br>
				<b><a href="https://2024.ieee-icra.org/" target="_blank">Conference on Computer Vision and Pattern Recognition (CVPR) 2024</a></b>
				<br>
				<font color="firebrick"><b>Highlight</b></font> 
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1802470520493146214">tl;dr</a> |
          		<a href="https://arxiv.org/abs/2405.09546">paper</a> |
          		<a href="https://behavior-vision-suite.github.io/">website</a> |
          		<a href="https://github.com/behavior-vision-suite/behavior-vision-suite.github.io">code</a> 
				</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<img class="img-fluid img-rounded" src="images/2023-minibh.png" style="border:0px solid black" alt="">
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI</font></b><br>
			<a href="https://www.linkedin.com/in/emily-jin-020/" target="_blank">Emily Jin</a>,  				
			<a href="https://jiahenghu.github.io/" target="_blank">Jiaheng Hu</a>, 
			<a href="https://www.linkedin.com/in/zhuoyi-huang/" target="_blank">Zhuoyi Huang</a>, 
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
			<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
			<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
			<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>
			<br>
			<b><a href="https://sites.google.com/view/aloe2023/home" target="_blank">Agent Learning in Open-Endedness (ALOE) Workshop at NeurIPS 2023</a></b>
			<br>
			<b><a href="https://aair-lab.github.io/genplan23/" target="_blank">Generalization in Planning (GenPlan) Workshop at NeurIPS 2023</a></b>
			<br>
			<a href="https://twitter.com/emilyzjin/status/1735788569921282537">tl;dr</a> |
			<a href="https://arxiv.org/abs/2310.01824">paper</a> |
			<a href="https://github.com/StanfordVL/mini_behavior/">code</a>
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<video class="img_responsive" style="border: 0px solid black; margin-top: 40pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
						<source src="videos/behavior.mp4" type="video/mp4">
					</video>
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation</font></b><br>
			<a href="https://www.chengshuli.me/" target="_blank">Chengshu Li</a>*, 
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>*, 
			<a href="https://jdw.ong/" target="_blank">Josiah Wong</a>*, 
			<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>*, 
			<a href="https://www.linkedin.com/in/sanjana-srivastava5/" target="_blank">Sanjana Srivastava</a>*, 
			<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>*, 
			<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>*, 
			<a href="https://www.gabrael.io/" target="_blank">Gabrael Levine</a>*, 
			<a href="https://wensi-ai.github.io/" target="_blank">Wensi Ai</a>*, 
			<a href="https://web.stanford.edu/~benjm/" target="_blank">Benjamin Jose Martinez</a>,
			<a href="https://www.linkedin.com/in/hangyin0226/" target="_blank">Hang Yin</a>,
			<a href="https://mlingelbach.com/" target="_blank">Michael Lingelbach</a>,
			<a href="https://mj-hwang.github.io/" target="_blank">Minjune Hwang</a>,
			<a href="https://misoshiruseijin.github.io/" target="_blank">Ayano Hiranaka</a>, 
			<a href="https://sujaygarlanka.com/" target="_blank">Sujay Garlanka</a>, 
			<a href="https://www.linkedin.com/in/arman-aydin/" target="_blank">Arman Aydin</a>, 
			<a href="https://knight-hennessy.stanford.edu/people/sharon-lee" target="_blank">Sharon Lee</a>, 
			<a href="https://web.stanford.edu/~jksun/" target="_blank">Jiankai Sun</a>,
			<a href="https://www.linkedin.com/in/mona-anvari/" target="_blank">Mona Anvari</a>,
			<a href="https://manasi-sharma.github.io/" target="_blank">Manasi Sharma</a>,
			<a href="https://www.linkedin.com/in/dhruvabansal2k/" target="_blank">Dhruva Bansal</a>,
			<a href="" target="_blank">Samuel Hunter</a>,
			<a href="https://kykim0.github.io/">Kyu-Young Kim</a>, 
			<a href="https://www.linkedin.com/in/alanlou/">Alan Lou</a>, 
			<a href="https://www.linkedin.com/in/caleb-matthews/">Caleb R Matthews</a>, 
			<a href="https://ivillar.github.io/">Ivan Villa-Renteria</a>, 
			<a href="https://www.linkedin.com/in/jerry-tang-b37026162/">Jerry Huayang Tang</a>, 
			<a href="">Claire Tang</a>, 
			<a href="https://fxia22.github.io/">Fei Xia</a>, 
			<a href="https://yunzhuli.github.io/">Yunzhu Li</a>, 
			<a href="https://www.linkedin.com/in/silvio-savarese-97b76114/">Silvio Savarese</a>, 
			<a href="https://web.stanford.edu/~hyo/Home.html">Hyowon Gweon</a>, 
			<a href="https://tml.stanford.edu/people/karen-liu" target="_blank">C. Karen Liu</a>,
			<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
			<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>     
			<br>
			<b><a href="https://corl2022.org/" target="_blank">Conference on Robot Learning (CoRL) 2022</a></b>
			<br>
			<font color="firebrick"><b>Best Paper Nomination</b></font> 
			<br>
			<font color="firebrick"><b>Oral Presentation</b></font> 
			<br>
			<a href="https://twitter.com/RuohanZhang76/status/1771019123298066812">tl;dr</a> |
			<a href="https://arxiv.org/abs/2403.09227">paper</a> |
			<a href="https://behavior.stanford.edu/">website</a> |
			<a href="https://github.com/StanfordVL/OmniGibson">code</a> |
			<a href="https://behavior.stanford.edu/omnigibson/getting_started/installation.html">guide</a> |
			<a href="https://discord.com/invite/bccR5vGFEx">support</a> 
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<img class="img-fluid img-rounded" src="images/2020-ahead.png" style="margin-top: 0pt; border:0px solid black" alt="">
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset</font></b><br>
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
			<a href="https://www.linkedin.com/in/calen-walshe-ab708364/" target="_blank">Calen Walshe</a>, 
			<a href="https://www.linkedin.com/in/zhuodeliu/" target="_blank">Zhuode Liu</a>, 
			<a href="https://guansuns.github.io/" target="_blank">Lin Guan</a>, 
			<a href="https://www.linkedin.com/in/karl-muller-025b5757/" target="_blank">Karl Muller</a>, 
			<a href="https://www.linkedin.com/in/jake-whritner-phd-366142189/" target="_blank">Jake Whritner</a>, 
			<a href="https://lucinezhang.github.io/" target="_blank">Luxin Zhang</a>, 
			<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>, 
			<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>
			<br>
			<b><a href="https://ijcai20.org/" target="_blank">AAAI Conference on Artificial Intelligence (AAAI) 2020</a></b>
			<br>
			<font color="firebrick"><b>Oral Presentation</b></font> at
			<b><a href="https://ijcai20.org/" target="_blank">AAAI Reinforcement Learning in Games Workshop 2020</a></b>
			<br>
			<a href="https://arxiv.org/abs/1903.06754">paper</a> | 
			<a href="https://zenodo.org/record/3451402#.XYPhzOtKhhE">data</a> |
			<a href="https://github.com/asaran/IL-CGL/tree/master/BC-CGL">code</a> |
			<a href="http://ai.stanford.edu/~zharu/publications/2020_AAAI_Dataset_poster.pdf">poster</a> |
			<a href="http://ai.stanford.edu/~zharu/publications/AAAI2020-RLG.pdf">talk</a>
			</div>
		</div><hr>


		<div id="lfh" style="padding-top: 80px; margin-top: -80px;">
		  <h5>Robot Learning from Humans</h5>
		</div><br>

		<div class="row">
				<div class="col-md-3">
					<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/brs.mp4" type="video/mp4">
						</video>
					</div>
				<div class="col-md-9">
				<b><font color="black">BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities</font></b><br>
				<a href="https://yunfanj.com/" target="_blank">Yunfan Jiang</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="https://jdw.ong/" target="_blank">Josiah Wong</a>,  
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>,
				<a href="https://yanjieze.com/" target="_blank">Yanjie Ze</a>,
				<a href="https://www.linkedin.com/in/hangyin0226/" target="_blank">Hang Yin</a>,
				<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>, 
				<a href="https://shurans.github.io/" target="_blank">Shuran Song</a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="" target="_blank">arxiv 2025</a></b>
				<br>
				<a href="https://x.com/RuohanZhang76/status/1900328330831991050">tl;dr</a> |
				<a href="https://arxiv.org/abs/2503.05652">paper</a> |
				<a href="https://behavior-robot-suite.github.io/">website</a> |
				<a href="https://behavior-robot-suite.github.io/docs/">documentation</a> |
				<a href="https://github.com/behavior-robot-suite/brs-ctrl">robot code</a> |
				<a href="https://github.com/behavior-robot-suite/brs-algo">algorithm code</a> 
				</div>
			</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/transic.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction</font></b><br>
				<a href="https://yunfanj.com/" target="_blank">Yunfan Jiang</a>, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="https://www.corl.org/" target="_blank">Conference on Robot Learning (CoRL) 2024</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1791603579159269835">tl;dr</a> |
				<a href="https://arxiv.org/abs/2405.10315">paper</a> |
				<a href="https://transic-robot.github.io/">website</a> |
				<a href="https://github.com/transic-robot/transic">code</a> 
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/dexcap.mp4" type="video/mp4">
					</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation</font></b><br>
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a>, 
				<a href="https://www.linkedin.com/in/weizhuo-ken-wang-ba4921119/" target="_blank">Weizhuo Wang</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://tml.stanford.edu/people/karen-liu" target="_blank">C. Karen Liu</a>
				<br>
				<b><a href="https://roboticsconference.org/" target="_blank">Robotics: Science and Systems (RSS) 2024</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1769533513940738127">tl;dr</a> |
				<a href="https://arxiv.org/abs/2403.07788">paper</a> |
				<a href="https://dex-cap.github.io/">website</a> |
				<a href="https://docs.google.com/document/d/1ANxSA_PctkqFf3xqAkyktgBgDWEbrFK7b1OnJe54ltw/edit#heading=h.yxlxo67jgfyx">hardware</a> |
				<a href="https://github.com/j96w/DexCap">code</a> 
				</div>
			</div><hr>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
						<source src="videos/telemoma.m4v" type="video/mp4">
					</video>
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation</font></b><br>
			<a href="https://shivindass.github.io/" target="_blank">Shivin Dass</a>, 
			<a href="https://wensi-ai.github.io/" target="_blank">Wensi Ai</a>, 
			<a href="https://yuqianjiang.us/" target="_blank">Yuqian Jiang</a>, 
			<a href="https://www.linkedin.com/in/imsamik/" target="_blank">Samik Singh</a>, 
			<a href="https://jiahenghu.github.io/" target="_blank">Jiaheng Hu</a>, 
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
			<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>, 
			<a href="https://babbatem.github.io/" target="_blank">Ben Abbatematteo</a>, 
			<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>
			<br>
			<b><a href="" target="_blank">arxiv 2024</a></b>
			<br>
			<a href="https://twitter.com/RuohanZhang76/status/1769535561998462982">tl;dr</a> |
			<a href="https://arxiv.org/abs/2403.07869">paper</a> |
			<a href="https://robin-lab.cs.utexas.edu/telemoma-web/">website</a> |
			<a href="https://github.com/UT-Austin-RobIn/telemoma">code</a> 
			</div>
		</div><hr>
		
		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/mimicplay.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Mimicplay: Long-horizon Imitation Learning by Watching Human Play</font></b><br>
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://jimfan.me/" target="_blank">Linxi Fan</a>, 
				<a href="https://web.stanford.edu/~jksun/" target="_blank">Jiankai Sun</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://faculty.cc.gatech.edu/~danfei/">Danfei Xu</a>, 
				<a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu</a>, 
				<a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a> 
				<br>
				<b><a href="https://www.corl2023.org/" target="_blank">Conference on Robot Learning (CoRL) 2023</a></b>
				<br>
				<font color="firebrick"><b>Finalist - Best Paper/Best Student Paper Awards <br>
					Finalist - Best Systems Paper Award <br> Oral Presentation</b></font> 
				<br>
				<a href="https://twitter.com/chenwang_j/status/1628792565385564160">tl;dr</a> |
				<a href="https://arxiv.org/abs/2302.12422">paper</a> |
				<a href="https://mimic-play.github.io/">website</a> |
				<a href="https://github.com/j96w/MimicPlay">code</a>
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/seed.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Primitive Skill-based Robot Learning from Human Evaluative Feedback</font></b><br>
				<a href="https://misoshiruseijin.github.io/" target="_blank">Ayano Hiranaka</a>*, 
				<a href="https://mj-hwang.github.io/" target="_blank">Minjune Hwang</a>*, 
				<a href="https://knight-hennessy.stanford.edu/people/sharon-lee" target="_blank">Sharon Lee</a>, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>
				<br>
				<b><a href="https://ieee-iros.org/" target="_blank">International Conference on Intelligent Robots and Systems (IROS) 2023</a></b>
				<br>
				<a href="https://arxiv.org/abs/2307.15801">paper</a> |
      			<a href="https://seediros23.github.io/">website</a> |
      			<a href="https://github.com/mj-hwang/seed">code</a>
				</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
						<source src="videos/dr-hrl.mp4" type="video/mp4">
					</video>
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">A Dual Representation Framework for Robot Learning with Human Guidance</font></b><br>
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>*, 
			<a href="https://www.linkedin.com/in/dhruvabansal2k/" target="_blank">Dhruva Bansal</a>*, 
			<a href="https://yih301.github.io/" target="_blank">Yilun Hao</a>*, 
			<a href="https://misoshiruseijin.github.io/" target="_blank">Ayano Hiranaka</a>, 
			<a href="https://gaojl19.github.io/">Jialu Gao</a>, 
			<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
			<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>, 
			<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
			<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
			<br>
			<b><a href="https://corl2022.org/" target="_blank">Conference on Robot Learning (CoRL) 2022</a></b>
			<br>
			<font color="firebrick"><b>Spotlight</b></font> at
			<b><a href="https://aligning-robot-human-representations.github.io/" target="_blank">Aligning Robot Representations with Humans Workshop at CoRL  2022</a></b>
			<br>
			<a href="https://proceedings.mlr.press/v205/zhang23a.html">paper</a> |
			<a href="https://sites.google.com/view/dr-hrl">website</a>
			</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2022-gradient.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">How to Train your Decision-Making AIs?</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.linkedin.com/in/dhruvabansal2k/" target="_blank">Dhruva Bansal</a>
				<br>
				<b><a href="https://thegradient.pub/" target="_blank">The Gradient 2022</a></b>
				<br>
				<a href="https://thegradient.pub/how-to-train-your-decision-making-ais/">link</a> | 
				<a href="https://ai.stanford.edu/~zharu/publications/2022_TheGradient_HumanTrainAI.pdf">free to read</a>
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2021-expand.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation</font></b><br>
				<a href="https://guansuns.github.io/" target="_blank">Lin Guan</a>, 
				<a href="https://famishedrover.github.io/" target="_blank">Mudit Verma</a>, 
				<a href="https://www.linkedin.com/in/suna-guo-680a96159/" target="_blank">Sihang Guo</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://rakaposhi.eas.asu.edu/" target="_blank">Subbarao Kambhampati</a> 
				<br>
				<b><a href="https://neurips.cc/Conferences/2021" target="_blank">Advances in Neural Information Processing Systems (NeurIPS) 2021</a></b>
				<br>
				<font color="firebrick"><b>Spotlight</b></font> 
				<br>
				<a href="https://arxiv.org/abs/2006.14804">paper</a> |
				<a href="https://github.com/GuanSuns/Simple-Human-in-the-Loop-ML-Interface">code</a> |
				<a href="https://papertalk.org/papertalks/37212">talk</a>
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2021-guidance.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Recent Advances in Leveraging Human Guidance for Sequential Decision-Making tasks</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>*, 				
				<a href="https://www.cs.utexas.edu/~faraztrb/" target="_blank">Faraz Torabi</a>*, 
				<a href="https://www.cs.utexas.edu/people/faculty-researchers/garrett-warnell" target="_blank">Garrett Warnell</a>, 
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
				<br>
				<b><a href="https://link.springer.com/journal/10458" target="_blank">Autonomous Agents and Multi-Agent Systems (JAAMAS) 2021</a></b>
				<br>
				<font color="firebrick"><b>Journal Paper (Survey)</b></font> 
				<br>
				<a href="https://link.springer.com/article/10.1007/s10458-021-09514-w">link</a> | 
				<a href="https://arxiv.org/abs/2107.05825">paper</a>
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2021-cgl.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Efficiently Guiding Imitation Learning Algorithms with Human Gaze</font></b><br>
				<a href="https://aihub.org/author/akankshasaran/" target="_blank">Akanksha Saran</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://eshort.github.io/" target="_blank">Elaine Schaertl Short</a>,
				<a href="https://people.cs.umass.edu/~sniekum/" target="_blank">Scott Niekum</a>
				<br>
				<b><a href="https://aamas2021.soton.ac.uk/" target="_blank">Autonomous Agents and Multi-Agent Systems (AAMAS) 2021</a></b>
				<br>
				<a href="https://arxiv.org/abs/2002.12500">paper</a> |
				<a href="https://github.com/asaran/IL-CGL">code</a> |
				<a href="https://drive.google.com/file/d/1bvfnkqFGF8O39yaF1QOcvhrOx2CpZyxX/view?usp=sharing">slides</a> |
				<a href="https://techxplore.com/news/2020-03-imitation-algorithms-human.html">Tech Xplore News</a>
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2020-gaze-survey.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Human Gaze Assisted Artificial Intelligence: A Review</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://aihub.org/author/akankshasaran/" target="_blank">Akanksha Saran</a>, 
				<a href="https://cranial-xix.github.io/" target="_blank">Bo Liu</a>, 
				<a href="https://zhuyifengzju.github.io/" target="_blank">Yifeng Zhu</a>, 
				<a href="https://www.linkedin.com/in/suna-guo-680a96159/" target="_blank">Sihang Guo</a>, 
				<a href="https://people.cs.umass.edu/~sniekum/" target="_blank">Scott Niekum</a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>, 
				<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>
				<br>
				<b><a href="https://ijcai20.org/" target="_blank">International Joint Conference on Artificial Intelligence (IJCAI) Survey Track 2020</a></b>
				<br>
				<a href="https://www.ijcai.org/Proceedings/2020/0689.pdf">paper</a>
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2019-guidance.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Leveraging Human Guidance for Deep Reinforcement Learning Tasks</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~faraztrb/" target="_blank">Faraz Torabi</a>, 
				<a href="https://guansuns.github.io/" target="_blank">Lin Guan</a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>, 
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
				<br>
				<b><a href="https://www.ijcai19.org/" target="_blank">International Joint Conference on Artificial Intelligence (IJCAI) Survey Track 2019</a></b>
				<br>
				<a href="https://arxiv.org/abs/1909.09906">paper</a>
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
						<source src="videos/agil.mp4" type="video/mp4">
					</video>
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">AGIL: Learning Attention from Human for Visuomotor Tasks</font></b><br>
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
			<a href="https://www.linkedin.com/in/zhuodeliu/" target="_blank">Zhuode Liu</a>, 
			<a href="https://lucinezhang.github.io/" target="_blank">Luxin Zhang</a>, 
			<a href="https://www.linkedin.com/in/jake-whritner-phd-366142189/" target="_blank">Jake Whritner</a>, 
			<a href="https://www.linkedin.com/in/karl-muller-025b5757/" target="_blank">Karl Muller</a>, 
			<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>, 
			<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>
			<br>
			<b><a href="https://eccv2018.org/" target="_blank">European Conference on Computer Vision (ECCV) 2018</a></b>
			<br>
			<a href="https://arxiv.org/abs/1806.03960">paper</a> |
			<a href="https://zenodo.org/record/3451402#.XYPhzOtKhhE">data</a> |
			<a href="https://github.com/asaran/IL-CGL/tree/master/BC-CGL">code</a>
			<br>
			<a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/5090/4963">AAAI 2019 Doctoral Consortium</a> |
			<a href="https://ojs.aaai.org/index.php/AAAI/article/view/12147">AAAI 2018 Student Abstract</a> |
			<a href="http://ai.stanford.edu/~zharu/publications/2017_NIPS_AGIL.pdf">NIPS 2017 CIAI workshop </a>
			<br>
			<a href="https://jov.arvojournals.org/article.aspx?articleid=2699524">VSS 2018 abstract</a> |
			<a href="https://ai.stanford.edu/~zharu/publications/VSS_2018_Modeling_Complex_Perception-Action_Choices.pdf">VSS 2018 talk</a> |
			<a href="https://ccn.societyconference.com/documents/1031/5928d74a68ed3f3c458a258b.pdf">CCN 2017 version</a>
		</div>
		</div><hr>



		<div id="hri" style="padding-top: 80px; margin-top: -80px;">
		  <h5>Human-Robot Interaction</h5>
		</div><br>
		
		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 10pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/noir.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">NOIR: Neural Signal Operated Intelligent Robots for Everyday Activities</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>*, 
				<a href="https://knight-hennessy.stanford.edu/people/sharon-lee" target="_blank">Sharon Lee</a>*, 
				<a href="https://mj-hwang.github.io/" target="_blank">Minjune Hwang</a>*,
				<a href="https://misoshiruseijin.github.io/" target="_blank">Ayano Hiranaka</a>*, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://wensi-ai.github.io/" target="_blank">Wensi Ai</a>, 
				<a href="https://www.linkedin.com/in/ryantjj/" target="_blank">Jin Jie Ryan Tan</a>,
				<a href="https://www.linkedin.com/in/shreya-gupta-08/" target="_blank">Shreya Gupta</a>, 
				<a href="https://yih301.github.io/" target="_blank">Yilun Hao</a>, 
				<a href="https://www.gabrael.io/" target="_blank">Gabrael Levine</a>, 
				<a href="https://ruohangao.github.io/" target="_blank">Ruohan Gao</a>, 
				<a href="https://psychology.stanford.edu/people/anthony-norcia" target="_blank">Anthony Norcia</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><a href="https://www.corl2023.org/" target="_blank">Conference on Robot Learning (CoRL) 2023</a></b>
				<br>
				<b>
				<font color="firebrick"><b>Oral Presentation</b></font> at
				<a href="https://yantianzha.github.io/crl.github.io/" target="_blank">Bridging the Gap between Cognitive Science and Robot Learning Workshop at CoRL 2023</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1720525179028406492">tl;dr</a> |
				<a href="https://arxiv.org/abs/2311.01454">paper</a> |
				<a href="https://noir-corl.github.io/">website</a> | 
				<a href="https://hai.stanford.edu/news/wearable-device-allows-humans-control-robots-brain-waves">Stanford HAI News</a> |
				<a href="https://w.mgtv.com/b/607797/20399189.html?t=videoshare&externalsource=v_play&tc=jXKKosRPSAN7&f=wxf&dc=9c199a33-461e-488e-bc06-3b252fe3e950">MangoTV interview (湖南卫视专访)</a> |
				<a href="https://www.deeplearning.ai/the-batch/issue-249/">The Batch</a> |
				<a href="https://braintitan.medium.com/noir-brain-powered-robots-for-daily-tasks-2cebd6ff6046">Medium</a>
				</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
						<source src="videos/voxposer.mp4" type="video/mp4">
					</video>
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models</font></b><br>
			<a href="https://wenlong.page/" target="_blank">Wenlong Huang</a>, <a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, <a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, <a href="https://yunzhuli.github.io/">Yunzhu Li</a>, <a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, <a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
			<br>
			<b><a href="https://www.corl2023.org/" target="_blank">Conference on Robot Learning (CoRL) 2023</a></b>
			<br>
			<font color="firebrick"><b>Oral Presentation</b></font> 
			<br>
			<a href="https://twitter.com/RuohanZhang76/status/1677376310811959296">tl;dr</a> |
			<a href="https://arxiv.org/abs/2307.05973">paper</a> |
			<a href="https://voxposer.github.io/">website</a> |
			<a href="https://github.com/huangwl18/VoxPoser">code</a>
			</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2018-qest.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Model Checking For Safe Navigation Among Humans</font></b><br>
				<a href="https://sjunges.github.io/" target="_blank">Sebastian Junges</a>,
				<a href="https://nilsjansen.org/" target="_blank">Nils Jansen</a>,
				<a href="https://www-i2.informatik.rwth-aachen.de/~katoen/" target="_blank">Joost-Pieter Katoen</a>,
				<a href="https://www.ae.utexas.edu/people/faculty/faculty-directory/topcu" target="_blank">Ufuk Topcu</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>
				<br>
				<b><a href="https://www.qest.org/qest2018/" target="_blank">International Conference on Quantitative Evaluation of SysTem (QEST) 2018</a></b>
				<br>
				<a href="https://link.springer.com/chapter/10.1007/978-3-319-99154-2_13">link</a> |
				<a href="https://link.springer.com/content/pdf/10.1007/978-3-319-99154-2.pdf">paper</a>
				</div>
		</div><hr>	

		<div id="fm" style="padding-top: 80px; margin-top: -80px;">
		  <h5>Foundation Models for Robotics</h5>
		</div><br>

		<div class="row">
				<div class="col-md-3">
					<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/uad.mp4" type="video/mp4">
						</video>
					</div>
				<div class="col-md-9">
				<b><font color="black">UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation</font></b><br>
				<a href="https://tangyihe.com/" target="_blank">Yihe Tang</a>, 
				<a href="https://wenlong.page/" target="_blank">Wenlong Huang</a>,
				<a href="https://www.wykac.com/" target="_blank">Yingke Wang</a>,
				<a href="https://www.chengshuli.me/" target="_blank">Chengshu Li</a>,
				<a href="https://www.linkedin.com/in/ryuan19/" target="_blank">Roy Yuan</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="https://2025.ieee-icra.org/" target="_blank">International Conference on Robotics and Automation (ICRA), 2025</a></b>
				<br>
				<font color="firebrick"><b>Best Paper Award Finalist</b></font> 
				<br>
				<font color="firebrick"><b>Best Paper Award Finalist in Robot Perception</b></font> 
				<br>
				<a href="https://x.com/RuohanZhang76/status/1924489528007426119">tl;dr</a> |
				<a href="https://arxiv.org/abs/2506.09284">paper</a> |
				<a href="https://unsup-affordance.github.io/">website</a> |
				<a href="https://github.com/TangYihe/unsup-affordance">code</a>
				</div>
			</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2024-eai.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
					</div>
				<div class="col-md-9">
				<b><font color="black">Embodied Agent Interface: A Single Line to Evaluate LLMs for Embodied Decision Making</font></b><br>
				<a href="https://limanling.github.io/">Manling Li</a>*, 
				Shiyu Zhao*, 
				<a href="https://qinengwang-aiden.github.io/">Qineng Wang</a>*, 
				<a href="https://jameskrw.github.io/">Kangrui Wang</a>*, 
				<a href="https://bryanzhou008.github.io/">Yu Zhou</a>*, 
				<a href="https://www.linkedin.com/in/sanjana-srivastava5/" target="_blank">Sanjana Srivastava</a>, 
				<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>, 
				<a href="https://www.linkedin.com/in/tonyhlee/">Tony Lee</a>, 
				<a href="https://www.cs.columbia.edu/~lierranli/">Li Erran Li</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="http://weiyuliu.com/" target="_blank">Weiyu Liu</a>, 
				<a href="https://cs.stanford.edu/~pliang/">Percy Liang</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>,
				<a href="https://jiayuanm.com/" target="_blank">Jiayuan Mao</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><a href="https://neurips.cc/" target="_blank">NeurIPS Datasets and Benchmarks Track 2024</a></b>
				<br>
				<font color="firebrick"><b>Oral Presentation</b></font> 
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1855401520793264302">tl;dr</a> |
				<a href="https://arxiv.org/abs/2410.07166">paper</a> |
				<a href="https://embodied-agent-interface.github.io/">website</a> |
				<a href="https://github.com/embodied-agent-interface/embodied-agent-interface">code</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/rekep.mp4" type="video/mp4">
						</video>
					</div>
				<div class="col-md-9">
				<b><font color="black">ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation</font></b><br>
				<a href="https://wenlong.page/" target="_blank">Wenlong Huang</a>,
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>*,
				<a href="https://yunzhuli.github.io/">Yunzhu Li</a>*,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="https://www.corl.org/" target="_blank">Conference on Robot Learning (CoRL) 2024</a></b>
				<br>
				<font color="firebrick"><b>Best Paper Award</b></font> at
				<b><a href="https://leap-workshop.github.io/" target="_blank">Learning Effective Abstractions for Planning (LEAP) Workshop at CoRL 2024</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1830534553267782005">tl;dr</a> |
				<a href="https://arxiv.org/abs/2409.01652">paper</a> |
				<a href="https://rekep-robot.github.io/">website</a> |
				<a href="https://github.com/huangwl18/ReKep">code</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/acdc.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Automated Creation of Digital Cousins for Robust Policy Learning</font></b><br>
				<a href="https://rogerdai1217.github.io/" target="_blank">Tianyuan Dai</a>,  
				<a href="https://jdw.ong/" target="_blank">Josiah Wong</a>,  
				<a href="https://yunfanj.com/" target="_blank">Yunfan Jiang</a>, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
				<br>
				<b><a href="https://www.corl.org/" target="_blank">Conference on Robot Learning (CoRL) 2024</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1844448510864916511">tl;dr</a> |
				<a href="https://arxiv.org/abs/2410.07408">paper</a> |
				<a href="https://digital-cousins.github.io/">website</a> |
				<a href="https://github.com/cremebrule/digital-cousins">code</a> 
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="images/2024-blade.png" style="border:0px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Learning Compositional Behaviors from Demonstration and Language</font></b><br>
				<a href="http://weiyuliu.com/" target="_blank">Weiyu Liu*</a>, 
				<a href="https://neilnie.com/" target="_blank">Neil Nie*</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://jiayuanm.com/" target="_blank">Jiayuan Mao†</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu†</a>
				<br>
				<b><a href="https://www.corl.org/" target="_blank">Conference on Robot Learning (CoRL) 2024</a></b>
				<br>
				<font color="firebrick"><b>Oral Presentation</b></font> at  
				<b><a href="https://leap-workshop.github.io/" target="_blank">Workshop on Learning Effective Abstractions for Planning (LEAP) at CoRL 2024</a></b>
				<br>
				<a href="https://x.com/Weiyu_Liu_/status/1854508231093092360">tl;dr</a> |
				<a href="https://blade-bot.github.io/blade.pdf">paper</a> |
				<a href="https://blade-bot.github.io/">website</a> |
				<a href="">code</a> 
				</div>
			</div><hr>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
						<source src="videos/voxposer.mp4" type="video/mp4">
					</video>
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models</font></b><br>
			<a href="https://wenlong.page/" target="_blank">Wenlong Huang</a>, <a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, <a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, <a href="https://yunzhuli.github.io/">Yunzhu Li</a>, <a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, <a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>
			<br>
			<b><a href="https://www.corl2023.org/" target="_blank">Conference on Robot Learning (CoRL) 2023</a></b>
			<br>
			<font color="firebrick"><b>Oral Presentation</b></font> 
			<br>
			<a href="https://twitter.com/RuohanZhang76/status/1677376310811959296">tl;dr</a> |
			<a href="https://arxiv.org/abs/2307.05973">paper</a> |
			<a href="https://voxposer.github.io/">website</a> |
			<a href="https://github.com/huangwl18/VoxPoser">code</a>
			</div>
		</div><hr>

		<div id="ns" style="padding-top: 80px; margin-top: -80px;">
		  <h5>Neuroscience and Cognitive Science</h5>
		</div><br>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2024-marple.jpg" style="border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Whodunnit? Inferring What Happened from Multimodal Evidence</font></b><br>
				<a href="https://sarahawu.github.io/">Sarah A. Wu</a>*,
                <a href="https://www.erikbrockbank.com/">Erik Brockbank</a>*,
                <a href="https://www.hannahcha.com/about">Hannah Cha</a>,
                <a href="https://janphilippfranken.github.io/">Jan-Philipp Fränken</a>,
				<a href="https://www.linkedin.com/in/emily-jin-020/" target="_blank">Emily Jin</a>, 
				<a href="https://www.linkedin.com/in/zhuoyi-huang/" target="_blank">Zhuoyi Huang</a>, 
                <a href="http://weiyuliu.com/">Weiyu Liu</a>,
                <a href="https://ai.stanford.edu/~zharu/"><u>Ruohan Zhang</u></a>,
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
                <a href="https://cicl.stanford.edu/member/tobias_gerstenberg/">Tobias Gerstenberg</a>
				<br>
				<b><a href="https://cognitivesciencesociety.org/cogsci-2024/" target="_blank">Proceedings of the Annual Meeting of the Cognitive Science Society (CogSci) 2024</a></b>
				<br>
				<a href="https://twitter.com/tobigerstenberg/status/1816363909588115515">tl;dr</a> |
				<a href="https://escholarship.org/uc/item/1nq5p6m8">paper</a> |
				<a href="https://github.com/cicl-stanford/whodunnit_multimodal_inference">code</a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 10pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/noir.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">NOIR: Neural Signal Operated Intelligent Robots for Everyday Activities</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>*, 
				<a href="https://knight-hennessy.stanford.edu/people/sharon-lee" target="_blank">Sharon Lee</a>*, 
				<a href="https://mj-hwang.github.io/" target="_blank">Minjune Hwang</a>*,
				<a href="https://misoshiruseijin.github.io/" target="_blank">Ayano Hiranaka</a>*, 
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>, 
				<a href="https://wensi-ai.github.io/" target="_blank">Wensi Ai</a>, 
				<a href="https://www.linkedin.com/in/ryantjj/" target="_blank">Jin Jie Ryan Tan</a>,
				<a href="https://www.linkedin.com/in/shreya-gupta-08/" target="_blank">Shreya Gupta</a>, 
				<a href="https://yih301.github.io/" target="_blank">Yilun Hao</a>, 
				<a href="https://www.gabrael.io/" target="_blank">Gabrael Levine</a>, 
				<a href="https://ruohangao.github.io/" target="_blank">Ruohan Gao</a>, 
				<a href="https://psychology.stanford.edu/people/anthony-norcia" target="_blank">Anthony Norcia</a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><a href="https://www.corl2023.org/" target="_blank">Conference on Robot Learning (CoRL) 2023</a></b>
				<br>
				<b>
				<font color="firebrick"><b>Oral Presentation</b></font> at
				<a href="https://yantianzha.github.io/crl.github.io/" target="_blank">Bridging the Gap between Cognitive Science and Robot Learning Workshop at CoRL 2023</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1720525179028406492">tl;dr</a> |
				<a href="https://arxiv.org/abs/2311.01454">paper</a> |
				<a href="https://noir-corl.github.io/">website</a> | 
				<a href="https://hai.stanford.edu/news/wearable-device-allows-humans-control-robots-brain-waves">Stanford HAI News</a> |
				<a href="https://w.mgtv.com/b/607797/20399189.html?t=videoshare&externalsource=v_play&tc=jXKKosRPSAN7&f=wxf&dc=9c199a33-461e-488e-bc06-3b252fe3e950">MangoTV interview (湖南卫视专访)</a> |
				<a href="https://www.deeplearning.ai/the-batch/issue-249/">The Batch</a> |
				<a href="https://braintitan.medium.com/noir-brain-powered-robots-for-daily-tasks-2cebd6ff6046">Medium</a>
				</div>
			</div><hr>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<img class="img-fluid img-rounded" src="images/2023-vivr.png" style="border:0px solid black" alt="">
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">Quantifying the Effect of Visual Impairments on Daily Activities in Virtual, Interactive Environments</font></b><br>
			<a href="https://wensi-ai.github.io/" target="_blank">Wensi Ai</a>, 
			<a href="https://knight-hennessy.stanford.edu/people/sharon-lee" target="_blank">Sharon Lee</a>,  
			<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
			<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>
			<br>
			<b><a href="https://cognitivesciencesociety.org/cogsci-2023/" target="_blank">Proceedings of the Annual Meeting of the Cognitive Science Society (CogSci) 2023</a></b>
			<br>
			<a href="https://escholarship.org/content/qt2sj3r0n2/qt2sj3r0n2.pdf">paper</a> |
			<a href="https://sites.google.com/view/vi-vr/?authuser=1">website</a>
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<img class="img-fluid img-rounded" src="images/2022-attention.png" style="margin-top: 0pt; border:0px solid black" alt="">
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">Selective Visual Attention during Public Speaking in an Immersive Context</font></b><br>
			<a href="https://www.paloaltou.edu/faculty/mikael-rubin-phd" target="_blank">Mikael Rubin</a>, 
			<a href="https://www.linkedin.com/in/suna-guo-680a96159/" target="_blank">Sihang Guo</a>, 
			<a href="https://www.linkedin.com/in/karl-muller-025b5757/" target="_blank">Karl Muller</a>, 
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
			<a href="https://labs.la.utexas.edu/telch/" target="_blank">Michael Telch</a>,
			<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>
			<br>
			<b><a href="https://link.springer.com/journal/13414" target="_blank">Attention, Perception, & Psychophysics 2022</a></b>
			<br>
			<font color="firebrick"><b>Journal Paper</b></font> 
			<br>
			<a href="https://link.springer.com/article/10.3758/s13414-021-02430-x">link</a> | 
			<a href="https://link.springer.com/content/pdf/10.3758/s13414-021-02430-x.pdf">paper</a> |
			<a href="https://osf.io/zg9xh/">data</a> |
			<a href="https://jov.arvojournals.org/article.aspx?articleid=2777608">VSS 2021 abstract</a>
			</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/mvha.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Machine versus Human Attention in Deep Reinforcement Learning Tasks</font></b><br>
				<a href="https://www.linkedin.com/in/suna-guo-680a96159/" target="_blank">Sihang Guo</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://cranial-xix.github.io/" target="_blank">Bo Liu</a>, 
				<a href="https://zhuyifengzju.github.io/" target="_blank">Yifeng Zhu</a>, 
				<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>,
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>,
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
				<br>
				<b><a href="https://neurips.cc/Conferences/2021" target="_blank">Advances in Neural Information Processing Systems (NeurIPS) 2021</a></b>
				<br>
      			<a href="https://arxiv.org/abs/2010.15942">paper</a> |
				<a href="https://zenodo.org/records/3451402">data</a> |  
      			<a href="https://jov.arvojournals.org/article.aspx?articleid=2776905">VSS 2021 abstract</a> 
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2021-tics.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">The Hierarchical Evolution in Human Vision Modeling</font></b><br>
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>
				<br>
				<b><a href="https://onlinelibrary.wiley.com/journal/17568765" target="_blank">Topics in Cognitive Sciences 2021</a></b>
				<br>
				<font color="firebrick"><b>Journal Paper</b></font> 
				<br>
				<a href="https://onlinelibrary.wiley.com/doi/10.1111/tops.12527">link</a> | 
				<a href="https://ai.stanford.edu/~zharu/publications/2020_TiCS_hier.pdf">free to read</a>
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2020-gamma.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Parallel Neural Processing with Gamma Frequency Latencies</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>
				<br>
				<b><a href="https://direct.mit.edu/neco" target="_blank">Neural Computation 2020</a></b>
				<br>
				<font color="firebrick"><b>Journal Paper</b></font> 
				<br>
				<a href="https://www.mitpressjournals.org/doi/full/10.1162/neco_a_01301">link</a> |
				<a href="https://ai.stanford.edu/~zharu/publications/neco_a_01301.pdf">free to read</a>
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2020-gamma2.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Cortical Spikes use Analog Sparse Coding</font></b><br>
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://lucgentetlab.wordpress.com/ " target="_blank">Luc Gentet</a>
				<br>
				<b><a href="https://www.biorxiv.org/" target="_blank">bioRxiv 2020</a></b>
				<br>
				<a href="https://www.biorxiv.org/content/10.1101/2020.10.19.331389v1">paper</a>
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="videos/mirl.mov" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Modeling Sensory-Motor Decisions in Natural Behavior</font></b><br>
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://shunzh.github.io/" target="_blank">Shun Zhang</a>, 
				<a href="https://www.linkedin.com/in/matthew-tong/" target="_blank">Matthew Tong</a>, 
				<a href="https://yuchencui.cc/" target="_blank">Yuchen Cui</a>, 
				<a href="https://www.pip.tu-darmstadt.de/ag_pip/index.en.jsp" target="_blank">Constatin Rothkopf</a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>, 
				<a href="https://liberalarts.utexas.edu/cps/faculty/mmh739" target="_blank">Mary Hayhoe</a>
				<br>
				<b><a href="https://journals.plos.org/ploscompbiol/" target="_blank">PLoS Computational Biology 2018</a></b>
				<br>
				<font color="firebrick"><b>Journal Paper</b></font> 
				<br>
				<a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006518&rev=2">link</a> |
				<a href="https://journals.plos.org/ploscompbiol/article/file?rev=2&id=10.1371/journal.pcbi.1006518&type=printable">paper</a> | 
				<a href="https://jov.arvojournals.org/article.aspx?articleid=2652133">VSS 2017 abstract</a> |
				<a href="https://ai.stanford.edu/~zharu/publications/2017_VSS_MIRL.pdf">VSS 2017 talk</a> |
				<a href="  http://ai.stanford.edu/~zharu/publications/2016_NETI_mirl.pdf">NETI 2016 poster</a>
		</div>
		</div><hr>

		<div id="ns" style="padding-top: 80px; margin-top: -80px;">
		  <h5>Embodied AI and Robotics</h5>
		</div><br>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2023-sgm.png" style="margin-top: 10pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Modeling Dynamic Environments with Scene Graph Memory</font></b><br>
				<a href="https://www.andreykurenkov.com/">Andrey Kurenkov</a>, 
				<a href="https://mlingelbach.com/" target="_blank">Michael Lingelbach</a>,
				<a href="https://tanmay-agarwal.com/" target="_blank">Tanmay Agarwal</a>, 
				<a href="https://www.chengshuli.me/" target="_blank">Chengshu Li</a>, 
				<a href="https://www.linkedin.com/in/emily-jin-020/" target="_blank">Emily Jin</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
				<a href="https://www.linkedin.com/in/silvio-savarese-97b76114/">Silvio Savarese</a>, 
				<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>
				<br>
				<b><a href="https://icml.cc/Conferences/2023" target="_blank">International Conference on Machine Learning (ICML) 2023</a></b>
				<br>
				<a href="https://twitter.com/andrey_kurenkov/status/1664742905062338560">tl;dr</a> |
				<a href="https://arxiv.org/abs/2305.17537">paper</a> |
				<a href="https://github.com/andreykurenkov/modeling_env_dynamics">code</a>
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2023-sgs.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Task-Driven Graph Attention for Hierarchical Relational Object Navigation</font></b><br>
				<a href="https://mlingelbach.com/" target="_blank">Michael Lingelbach</a>, 
				<a href="https://www.chengshuli.me/" target="_blank">Chengshu Li</a>, 
				<a href="https://mj-hwang.github.io/" target="_blank">Minjune Hwang</a>,
				<a href="https://www.andreykurenkov.com/">Andrey Kurenkov</a>, 
				<a href="https://www.linkedin.com/in/alanlou/">Alan Lou</a>,
				<a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><a href="https://www.icra2023.org/" target="_blank">International Conference on Robotics and Automation (ICRA) 2023</a></b>
				<br>
				<a href="https://arxiv.org/abs/2306.13760">paper</a> |
				<a href="https://github.com/mjlbach/ssg">code</a>
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2022-imma.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Interaction Modeling with Multiplex Attention</font></b><br>
				<a href="https://sunfanyun.com/">Fan-Yun Sun</a>, 
				<a href="https://ikauvar.github.io/">Isaac Kauvar</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://jiachenli94.github.io/">Jiachen Li</a>, 
				<a href="https://mykel.kochenderfer.com/">Mykel J. Kochenderfer</a>, 
				<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>, 
				<a href="https://www.autonomousagents.stanford.edu/">Nick Haber</a>
				<br>
				<b><a href="https://neurips.cc/Conferences/2022" target="_blank">Advances in Neural Information Processing Systems (NeurIPS) 2022</a></b>
				<br>
				<a href="https://twitter.com/RuohanZhang76/status/1579529563616342016">tl;dr</a> |
				<a href="https://arxiv.org/abs/2208.10660">paper</a> |
				<a href="https://sites.google.com/view/dr-hrl">website</a> |
				<a href="https://github.com/fanyun-sun/IMMA">code</a>
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2020-attrl.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">An Initial Attempt of Combining Visual Selective Attention with Deep Reinforcement Learning</font></b><br>
				<a href="https://www.linkedin.com/in/liuyuezhang/" target="_blank">Liu Yuezhang</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>
				<br>
				<b><a href="https://arxiv.org/" target="_blank">arxiv 2020</a></b>
				<br>
				<a href="https://arxiv.org/abs/1811.04407">paper</a>
				</div>
		</div><hr>


		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
						<source src="videos/robocup.mp4" type="video/mp4">
					</video>
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">UT Austin Villa: Project-Driven Research in AI and Robotics</font></b><br>
			<a href="https://www.cs.utexas.edu/~katie/index.html" target="_blank">Katie Genter</a>, 
			<a href="https://www.linkedin.com/in/patrick-macalpine-0ba700a/" target="_blank">Patrick MacAlpine</a>, 
			<a href="https://www.linkedin.com/in/jacob-menashe-10898b83/" target="_blank">Jacob Menashe</a>, 
			<a href="https://pages.cs.wisc.edu/~jphanna/" target="_blank">Josiah Hanna</a>, 
			<a href="https://eladlieb.weebly.com/" target="_blank">Elad Liebman</a>, 
			<a href="https://www.cs.utexas.edu/~sanmit/" target="_blank">Sanmit Narvekar</a>, 
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
			<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
			<br>
			<b><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9670" target="_blank">IEEE Intelligent Systems 31(2) 2016</a></b>
			<br>
			<font color="firebrick"><b>Journal Paper</b></font> 
			<br>
			<a href="http://ieeexplore.ieee.org/document/7435178/?arnumber=7435178">link</a> |
			<a href="https://spl.robocup.org/open-source/">code</a> |
			<a href="https://www.cs.utexas.edu/~AustinVilla/?p=nao"> UT Austin Villa</a>
		</div>
		</div><hr>


		<div class="row">
			<div class="col-md-3">
				<video class="img_responsive" style="border: 0px solid black; margin-top: 0pt; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
						<source src="videos/aim.mp4" type="video/mp4">
				</video>
			</div>
			<div class="col-md-9">
			<b><font color="black">Decision-Making Policies for Heterogeneous Autonomous Multi-Agent Systems with Safety Constraints</font></b><br>
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
			<a href="https://cse.umn.edu/aem/yue-yu" target="_blank">Yue Yu</a>,
			<a href="https://www.linkedin.com/in/mahmoud-el-chamie-3a9a6b25/" target="_blank">Mahmoud El Chamie</a>,
			<a href="https://uwacl.com/" target="_blank">Behçet Açikmese</a>,
			<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>
			<br>
			<b><a href="https://ijcai-16.org/" target="_blank">International Joint Conference on Artificial Intelligence (IJCAI) 2016</a></b>
			<br>
			<a href="http://www.ijcai.org/Proceedings/16/Papers/084.pdf">paper</a>
		</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<img class="img-fluid img-rounded" src="images/2016-msy.png" style="margin-top: 0pt; border:0px solid black" alt="">
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">Maximum Sustainable Yield Problem for Robot Foraging and Construction System</font></b><br>
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
			<a href="https://www.youtube.com/@zhaosong2031" target="_blank">Zhao Song</a>				 
			<br>
			<b><a href="https://ijcai-16.org/" target="_blank">International Joint Conference on Artificial Intelligence (IJCAI) 2016</a></b>
			<br>
			<a href="http://www.ijcai.org/Proceedings/16/Papers/387.pdf">paper</a>
		</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<img class="img-fluid img-rounded" src="images/austin-villa.png" style="margin-top: 0pt; border:0px solid black" alt="">
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">UT Austin Villa 2017 Team Description Paper for the Standard Platform League</font></b><br>
			<a href="https://www.cs.utexas.edu/~katie/index.html" target="_blank">Katie Genter</a>, 				
			<a href="https://pages.cs.wisc.edu/~jphanna/" target="_blank">Josiah Hanna</a>,
			<a href="http://joshkelle.com/" target="_blank">Josh Kelle</a>, 
			<a href="https://eladlieb.weebly.com/" target="_blank">Elad Liebman</a>, 
			<a href="https://www.linkedin.com/in/jacob-menashe-10898b83/" target="_blank">Jacob Menashe</a>, 
			<a href="https://www.cs.utexas.edu/~sanmit/" target="_blank">Sanmit Narvekar</a>, 
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
			<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
			<br>
			<b><a href="https://www.robocup2017.org/lander" target="_blank">RoboCup 2017</a></b>
			<br>
			<a href="https://2017.robocup.org/file/symposium/soccer_std_plf/UT_TDP17.pdf">paper</a> |
			<a href="https://www.cs.utexas.edu/~AustinVilla/?p=nao"> UT Austin Villa</a>
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<img class="img-fluid img-rounded" src="images/austin-villa.png" style="margin-top: 0pt; border:0px solid black" alt="">
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">UT Austin Villa 2016 Team Description Paper for the Standard Platform League</font></b><br>
			<a href="https://www.cs.utexas.edu/~katie/index.html" target="_blank">Katie Genter</a>, 				
			<a href="https://pages.cs.wisc.edu/~jphanna/" target="_blank">Josiah Hanna</a>,
			<a href="http://joshkelle.com/" target="_blank">Josh Kelle</a>, 
			<a href="https://eladlieb.weebly.com/" target="_blank">Elad Liebman</a>, 
			<a href="https://www.linkedin.com/in/jacob-menashe-10898b83/" target="_blank">Jacob Menashe</a>, 
			<a href="https://www.cs.utexas.edu/~sanmit/" target="_blank">Sanmit Narvekar</a>, 
			<a href="" target="_blank">Rishi Shah</a>, 
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
			<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
			<br>
			<b><a href="https://2016.robocup.org/web/index-2.html" target="_blank">RoboCup 2016</a></b>
			<br>
			<a href="https://2016.robocup.org/web/images/robocup_2016_spl_tdp_utaustinvilla.pdf">paper</a> |
			<a href="https://www.cs.utexas.edu/~AustinVilla/?p=nao"> UT Austin Villa</a>
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<img class="img-fluid img-rounded" src="images/austin-villa.png" style="margin-top: 0pt; border:0px solid black" alt="">
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">UT Austin Villa 2015 Team Description Paper for the Standard Platform League</font></b><br>
			<a href="https://www.cs.utexas.edu/~katie/index.html" target="_blank">Katie Genter</a>, 
			<a href="https://pages.cs.wisc.edu/~jphanna/" target="_blank">Josiah Hanna</a>, 
			<a href="https://eladlieb.weebly.com/" target="_blank">Elad Liebman</a>, 
			<a href="https://www.linkedin.com/in/jacob-menashe-10898b83/" target="_blank">Jacob Menashe</a>,
			<a href="https://www.cs.utexas.edu/~sanmit/" target="_blank">Sanmit Narvekar</a>, 
			<a href="https://www.eecs.tufts.edu/~jsinapov/" target="_blank">Jivko Sinapov</a>, 
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
			<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>      
			<br>
			<b><a href="https://www.robocup2015.org/" target="_blank">RoboCup 2015</a></b>
			<br>
			<a href="http://robocup2015.oss-cn-shenzhen.aliyuncs.com/TeamDescriptionPapers/StandardPlatform/RoboCup_Symposium_2015_submission_72.pdf">paper</a> |
			<a href="https://www.cs.utexas.edu/~AustinVilla/?p=nao"> UT Austin Villa</a>
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<img class="img-fluid img-rounded" src="images/2015-mrl.png" style="margin-top: 0pt; border:0px solid black" alt="">
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">Global Policy Construction in Modular Reinforcement Learning</font></b><br>
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
			<a href="https://www.youtube.com/@zhaosong2031" target="_blank">Zhao Song</a>,				 
			<a href="https://www.cs.utexas.edu/~dana/" target="_blank">Dana Ballard</a>
			<br>
			<b><a href="https://aaai.org/conference/aaai/aaai15/" target="_blank">AAAI Conference on Artificial Intelligence (AAAI) Student Abstract 2015</a></b>
			<br>
			<a href="https://cdn.aaai.org/ojs/9736/9736-13-13264-1-2-20201228.pdf">paper</a>
			</div>
		</div><hr>


		<div id="ns" style="padding-top: 80px; margin-top: -80px;">
		  <h5>Computer Vision</h5>
		</div><br>

		<div class="row">
			<div class="col-md-3">
				<div class="video-container" style="overflow: hidden;">
					<img class="img-fluid img-rounded" src="images/2023-finv.png" style="margin-top: 10pt; border:0px solid black" alt="">
				</div>
			</div>
			<div class="col-md-9">
			<b><font color="black">Partial-View Object View Synthesis via Filtering Inversion</font></b><br>
			<a href="https://sunfanyun.com/" target="_blank">Fan-Yun Sun</a>,
			<a href="https://research.nvidia.com/person/jonathan-tremblay" target="_blank">Jonathan Tremblay</a>,
			<a href="https://www.cs.cornell.edu/~valts/" target="_blank">Valts Blukis</a>,
			<a href="https://kevin-thankyou-lin.github.io/" target="_blank">Kevin Lin</a>,
			<a href="https://faculty.cc.gatech.edu/~danfei/" target="_blank">Danfei Xu</a>,
			<a href="https://www.borisivanovic.com/" target="_blank">Boris Ivanovic</a>,
			<a href="https://karkus.tilda.ws/" target="_blank">Peter Karkus</a>,
			<a href="https://cecas.clemson.edu/~stb/" target="_blank">Stan Birchfield</a>,
			<a href="https://homes.cs.washington.edu/~fox/" target="_blank">Dieter Fox</a>,
			<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>,
			<a href="https://yunzhuli.github.io/">Yunzhu Li</a>,
			<a href="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
			<a href="https://research.nvidia.com/person/marco-pavone" target="_blank">Marco Pavone</a>,
			<a href="https://ed.stanford.edu/faculty/nhaber" target="_blank">Nick Haber</a>
			<br>
			<b><a href="https://3dvconf.github.io/2024/" target="_blank">International Conference on 3D Vision (3DV) 2024</a></b>
			<br>
			<font color="firebrick"><b>Spotlight</b></font> 
			<br>
			<a href="https://arxiv.org/abs/2304.00673">paper</a> |
			<a href="https://cs.stanford.edu/~sunfanyun/finv/">website</a> |
			<a href="https://github.com/sunfanyunn/FINV">code</a>
			</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2017-robocup-ball.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Fast and Precise Black and White Ball Detection for RoboCup Soccer</font></b><br>
				<a href="https://www.linkedin.com/in/jacob-menashe-10898b83/" target="_blank">Jacob Menashe</a>,
				<a href="http://joshkelle.com/" target="_blank">Josh Kelle</a>,
				<a href="https://www.cs.utexas.edu/~katie/index.html" target="_blank">Katie Genter</a>,
				<a href="https://pages.cs.wisc.edu/~jphanna/" target="_blank">Josiah Hanna</a>,
				<a href="https://eladlieb.weebly.com/" target="_blank">Elad Liebman</a>,
				<a href="https://www.cs.utexas.edu/~sanmit/" target="_blank">Sanmit Narvekar</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>
				<br>
				<b><a href="https://2017.robocup.org/eng/symposium.html" target="_blank">RoboCup Symposium 2017</a></b>
				<br>
				<a href="https://2017.robocup.org/file/symposium/RoboCup_Symposium_2017_paper_20.pdf">paper</a> |
				<a href="https://spl.robocup.org/open-source/">code</a> |
				<a href="https://www.cs.utexas.edu/~AustinVilla/?p=nao"> UT Austin Villa</a>
				</div>
		</div><hr>

		<div id="ns" style="padding-top: 80px; margin-top: -80px;">
		  <h5>Machine Learning</h5>
		</div><br>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2017-gdmm.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Greedy Direction Method of Multiplier for MAP Inference of Large Output Domain</font></b><br>
				<a href="http://wueh4.q-huan.link/english/dstd-en/dstd-gxy/dstd-gxy-rgznysjkx/202403/50277.html" target="_blank">Xiangru Huang</a>, 
				<a href="http://ianyen.site/" target="_blank">Ian E.H. Yen</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.utexas.edu/~huangqx/" target="_blank">Qixing Huang</a>, 
				<a href="https://www.cs.cmu.edu/~pradeepr/" target="_blank">Pradeep Ravikumar</a>, 
				<a href="https://www.linkedin.com/in/inderjit-dhillon-a20888b0/" target="_blank">Inderjit S. Dhillon</a> 
				<br>
				<b><a href="https://aistats.org/aistats2017/" target="_blank">Artificial Intelligence and Statistics (AISTATS) 2017</a></b>
				<br>
				<a href="http://proceedings.mlr.press/v54/huang17a">paper</a> |
				<a href="https://github.com/xiangruhuang/FastStructPred">code</a>
				</div>
		</div><hr>

		<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<img class="img-fluid img-rounded" src="images/2016-gdmm.png" style="margin-top: 0pt; border:0px solid black" alt="">
					</div>
				</div>
				<div class="col-md-9">
				<b><font color="black">Dual Decomposed Learning with Factorwise Oracle for Structural SVM of Large Output Domain</font></b><br>
				<a href="http://ianyen.site/" target="_blank">Ian E.H. Yen</a>, 
				<a href="http://wueh4.q-huan.link/english/dstd-en/dstd-gxy/dstd-gxy-rgznysjkx/202403/50277.html" target="_blank">Xiangru Huang</a>, 
				<a href="https://www.linkedin.com/in/kai-zhong-29089436/" target="_blank">Kai Zhong</a>, 
				<a href="https://ai.stanford.edu/~zharu/" target="_blank"><u>Ruohan Zhang</u></a>, 
				<a href="https://www.cs.cmu.edu/~pradeepr/" target="_blank">Pradeep Ravikumar</a>, 
				<a href="https://www.linkedin.com/in/inderjit-dhillon-a20888b0/" target="_blank">Inderjit S. Dhillon</a> 
				<br>
				<b><a href="https://neurips.cc/Conferences/2016" target="_blank">Advances in Neural Information Processing Systems (NIPS) 2016</a></b>
				<br>
				<a href="https://proceedings.neurips.cc/paper_files/paper/2016/hash/7e83722522e8aeb7512b7075311316b7-Abstract.html">paper</a>
				</div>
		</div><hr>

		</script> 



		<br>
		<div class="container">
		<h3 id="Awards" style="padding-top: 80px; margin-top: -80px;">Awards</h3>
		<ul>
			<li>2022-24 Wu Tsai Human Performance Alliance Fellowship, Stanford University</li>
			<li>2019 University Continuing Fellowship, The University of Texas at Austin</li>
			<li>2019 AAAI/SIGAI Doctoral Consortium Scholarship</li>
			<li>2018 General International Student and Scholar Services Financial Aid Award, The University of Texas at Austin </li>
			<li>2017-18 Google AR/VR Research Award</li>
			<li>2014-17 College Continuing Fellowship, The University of Texas at Austin</li>
			<li>2011 PSI CHI International Honor Society in Psychology</li>
			<li>2008-12 Rhodes College Grants</li>
			<li>2009-10 Rhodes College Honor Roll</li>
			<li>2008-09 Rhodes College Dean’s List</li>
		</ul>
		</div><br>	

		<div class="container">
		<h3 id="Service" style="padding-top: 80px; margin-top: -80px;">Service and Teaching</h3>
		<ul>
			<li>Organizer, ICCV, ECCV, NeurIPS workshops</li>
			<li>Conference review, CoRL, RSS, IROS, ICRA, NeurIPS</li>
			<li>Journal review, Frontiers in Artificial Intelligence</li>
			<li>Journal review, Journal of Vision</li>
			<li>Book review, Chapman and Hall/CRC Press</li>
		</ul>
		<ul>
			<li>Instructor, <a href="https://cs231n.stanford.edu/index.html">CS231n: Deep Learning for Computer Vision</a> 2023, Stanford University</li>
			<li>Teaching Assistant, Computational Brain 2019, The University of Texas at Austin</li>
			<li>Teaching Assistant, Machine Learning 2015, The University of Texas at Austin</li>
			<li>Teaching Assistant, Discrete Mathematics (Honor) 2014, The University of Texas at Austin</li>
			<li>Teaching Assistant, Discrete Mathematics 2013, The University of Texas at Austin</li>
			<li>Teaching Assistant, Logic, Sets, and Functions 2013, The University of Texas at Austin</li>
			<li>Teaching Assistant, Qualitative Methods 2012, Rhodes College</li>
		</ul>
		</div><br>



	<div class="container">
		<h3 id="Mentoring" style="padding-top: 80px; margin-top: -80px;">Mentoring</h3>
		<ul>
			<li><a href="https://www.wykac.com/">Yingke Wang</a> (Master, Stanford University)</li>
			<li><a href="https://www.linkedin.com/in/tashakim/">Tasha Kim</a> (Master, Stanford University)</li>
			<li><a href="https://www.linkedin.com/in/ryantjj/">Ryan Tan</a> (Master, Stanford University &rarr; Glean)</li>
			<li><a href="https://knight-hennessy.stanford.edu/people/sharon-lee">Sharon Lee</a> (Master, Stanford University &rarr; PhD, Stanford University)</li>
			<li><a href="https://www.linkedin.com/in/shuojia-fu-60573a236/?trk=public_profile_browsemap">Shuojia Fu</a> (Master, Stanford University &rarr; PhD, Stanford University)</li>
			<li><a href="https://yih301.github.io/">Yilun Hao</a> (Master, Stanford University &rarr; PhD, MIT)</li>
			<li><a href="https://gaojl19.github.io/">Jialu Gao</a> (Undergrad, Tsinghua University &rarr; Master, Carnegie Mellon University)</li>
			<li><a href="https://wensi-ai.github.io/">Wensi Ai</a> (Undergrad, UCLA &rarr; Master, Stanford University)</li>
			<li><a href="https://www.linkedin.com/in/emily-jin-020/">Emily Jin</a> (Undergrad, Stanford University &rarr; Master, Stanford University)</li>
			<li><a href="https://www.linkedin.com/in/zhuoyi-huang/">Zhuoyi Huang</a> (Master, Stanford University &rarr; Microsoft AI)</li>
			<li><a href="https://mj-hwang.github.io/">Minjune Hwang</a> (Master, Stanford University &rarr; PhD, USC)</li>
			<li><a href="https://misoshiruseijin.github.io/">Ayano Hiranaka</a> (Master, Stanford University &rarr; PhD, USC)</li>
			<li><a href="https://misoshiruseijin.github.io/">Dhruva Bansal</a> (Master, Stanford University &rarr; Google DeepMind)</li>
			<li><a href="https://www.linkedin.com/in/suna-guo-680a96159/">Sihang Guo</a> (Master, The University of Texas at Austin &rarr; PhD, The University of Texas at Austin)</li>
			<li><a href="https://www.dwliang.com/">Dawei Liang</a> (Master, The University of Texas at Austin &rarr; PhD, The University of Texas at Austin)</li>
			<li><a href="https://guansuns.github.io/">Lin Guan</a> (Undergrad, The University of Texas at Austin &rarr; PhD, Arizona State University)</li>
			<li><a href="https://www.linkedin.com/in/liuyuezhang/">Yuezhang Liu</a> (Undergrad, Tsinghua University &rarr; PhD, The University of Texas at Austin)</li>
			<li><a href="https://lucinezhang.github.io/">Luxin Zhang</a> (Undergrad, Peking University &rarr; Master, Carnegie Mellon University)</li>
			<li><a href="https://www.linkedin.com/in/zhuodeliu/">Zhuode Liu</a> (Master, The University of Texas at Austin &rarr; Google)</li>
		</ul>
	</div><br>

	<div class="container">
		<hr>
		<center>
			<footer>
				<p>&copy; Stanford University 2024</p>
			</footer>
		</center>
	</div>
	<!-- /container -->

	<!-- Bootstrap core JavaScript -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script>showPubs(0);</script>
	<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>
	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>
</body>

</html>
