<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ruohan Zhang</title>
  
  <meta name="author" content="Ruohan Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ruohan Zhang</name>
              </p>
              <p>I am a researcher at <a href="http://svl.stanford.edu/">Stanford Vision and Learning Lab (SVL)</a>, as well as a <a href="https://humanperformancealliance.org/">Wu Tsai Human Performance Alliance</a> Fellow. I work on robotics, human-robot interaction, brain-machine interface, neuroscience, and art.
              </p>
              <p>
                I am currently working with <a href="http://svl.stanford.edu/people/">Prof. Fei-Fei Li, Prof. Jiajun Wu, and Prof. Silvio Savarese</a>. I received my Ph.D. from The University of Texas at Austin, advised by <a href="http://www.cs.utexas.edu/~dana/">Prof. Dana Ballard</a> and <a href="http://www.utexas.edu/cola/centers/cps/faculty/mmh739">Prof. Mary Hayhoe</a>. 
              </p>
              <p style="text-align:center">
                <a href="mailto:zharu@stanford.edu">Email</a> &nbsp/&nbsp
                <a href="data/Ruohan_CV.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/Ruohan_bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=-bqvNWoAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/RuohanZhang76">Twitter</a> 
                <!-- <a href="https://github.com/corgiTrax">Github</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile.jpg"><img style="width:90%;max-width:90%" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My longterm research interest is human-centered artificial intelligence: Understanding human intelligence for developing biologically-inspired AI algorithms, as well as making AIs more compatible with humans. Recently I am focusing on human-centered robotics: developing robotic solutions (systems and algorithms) that enhance human wellbeing, with data-driven approaches.
                <br><br>
                My most recent works fall into the following categories: <br>
                1. Identifying tasks that matter to humans, and building infrastructure to support robotics research to solve these tasks: 
                <a href="https://behavior.stanford.edu/"><b>BEHAVIOR</b></a> |
                <a href="https://behavior-vision-suite.github.io/">BEHAVIOR Vision Suite</a>.
                <!-- DigitalCousin | Econ, Elderly, robotRubric -->
                <br>
                2. Leveraging large-scale and diverse human data for robot learning:
                <a href="https://mimic-play.github.io/"><b>MimicPlay</b></a> | 
                <a href="https://dex-cap.github.io/"><b>DexCap</b></a> | 
                <a href="https://transic-robot.github.io/"><b>TRANSIC</b></a> |
                <a href="https://seediros23.github.io/">SEED</a> |
                <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6161/">AGIL</a>.
                <!--RTX-->
                <br>
                3. Widening the communication pipeline between humans and robots:
                <a href="https://noir-corl.github.io/"><b>NOIR</b></a> |
                <a href="https://voxposer.github.io/"><b>VoxPoser</b></a>.
                <!--NOIR-implant, NOIR-EEG, VoxPoser2, HRAlignment-->
                <br>
                4. Understanding human brain and behaviors:
                <a href="https://direct.mit.edu/neco/article/32/9/1635/95604/Parallel-Neural-Multiprocessing-with-Gamma">GSM</a> |
                <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006518&rev=2">ModularIRL</a> |
                <a href="https://sites.google.com/view/vi-vr/?authuser=1">VIVR</a>.
                <!--MARPLE, <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/d58e2f077670f4de9cd7963c857f2534-Paper.pdf">MvHA</a>, IMMA-->
                <!-- 5. Human knowledge-driven robotics
                RoboArt, GPT-aff, KDM, LLMRobotBenchmark-->
                <br><br>
                <font color="red"><strong>I will be on academic job market in 2025. </strong></font>

              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rs.png" alt="clean-usnob" width="210" height="160">
            </td>
          <td style="padding:0px;width:75%;vertical-align:middle">
              <papertitle>Understanding Human Intelligence in the Era of Artificial Intelligence.</papertitle>
            </a>
            <br>
            <u>Ruohan Zhang</u>
            <br>
            <em>Research Summary</em>, 2020 
            <br>
            <a href="https://ai.stanford.edu/~zharu/publications/rs.pdf">paper</a>
          </td>
        </tr>
<!-- 
        TODOs: MARPLEx2, LLMeval, OlderAdults, ShapeCraft
        Longterm TODOs: NOIR-EEG2 journal; NOIR-implant journal, LLM-reward, RoboArt, GPTaff, BH-Econ, RobotRubric -->

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2024-rekep.png" alt="clean-usnob" width="210" height="120">
          </td>
        <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation</papertitle>
          </a>
          <br>
          Wenlong Huang, Chen Wang, Yunzhu Li, <u>Ruohan Zhang</u>, Li Fei-Fei
          <br>
          <em>Conference on Robot Learning (CoRL)</em>, 2024
          <br>
          <a href="https://twitter.com/RuohanZhang76/status/1830534553267782005">tl;dr</a> |
          <a href="https://arxiv.org/abs/2409.01652">paper</a> |
          <a href="https://rekep-robot.github.io/">website</a> |
          <a href="https://github.com/huangwl18/ReKep">code</a> 
        </td>
      </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2024-transic.png" alt="clean-usnob" width="210" height="120">
          </td>
        <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction</papertitle>
          </a>
          <br>
          Yunfan Jiang, Chen Wang, <u>Ruohan Zhang</u>, Jiajun Wu, Li Fei-Fei
          <br>
          <em>Conference on Robot Learning (CoRL)</em>, 2024
          <br>
          <a href="https://twitter.com/RuohanZhang76/status/1791603579159269835">tl;dr</a> |
          <a href="https://arxiv.org/abs/2405.10315">paper</a> |
          <a href="https://transic-robot.github.io/">website</a> |
          <a href="https://github.com/transic-robot/transic">code</a> 
        </td>
      </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/2024-acdc.png" alt="clean-usnob" width="210" height="110">
      </td>
    <td style="padding:0px;width:75%;vertical-align:middle">
        <papertitle>ACDC: Automated Creation of Digital Cousins for Robust Policy Learning</papertitle>
      </a>
      <br>
      Tianyuan Dai, Josiah Wong, Yunfan Jiang, Chen Wang, Cem Gokmen, <u>Ruohan Zhang</u>, Jiajun Wu, Li Fei-Fei
      <br>
      <em>Conference on Robot Learning (CoRL)</em>, 2024
      <br>
      <a href="">tl;dr</a> |
      <a href="https://openreview.net/forum?id=7c5rAY8oU3">paper</a> |
      <a href="">website</a> |
      <a href="">code</a> 
    </td>
  </tr>

  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/2024-blade.png" alt="clean-usnob" width="210" height="110">
    </td>
  <td style="padding:0px;width:75%;vertical-align:middle">
      <papertitle>Learning Compositional Behaviors from Demonstration and Language</papertitle>
    </a>
    <br>
    Weiyu Liu, Neil Nie, Jiayuan Mao, <u>Ruohan Zhang</u>, Jiajun Wu
    <br>
    <em>Conference on Robot Learning (CoRL)</em>, 2024
    <br>
    <a href="">tl;dr</a> |
    <a href="https://openreview.net/forum?id=fR1rCXjCQX">paper</a> |
    <a href="">website</a> |
    <a href="">code</a> 
  </td>
</tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/2024-rtx.png" alt="clean-usnob" width="210" height="110">
        </td>
      <td style="padding:0px;width:75%;vertical-align:middle">
          <papertitle>Open X-Embodiment: Robotic Learning Datasets and RT-X Models</papertitle>
        </a>
        <br>
        Open X-Embodiment Collaboration
        <br>
        <em>ICRA</em>, 2024 &nbsp <font color="red"><strong>(Best Paper Award)</strong></font>
        <br>
        <a href="https://twitter.com/QuanVng/status/1709209020341669988">tl;dr</a> |
        <a href="https://arxiv.org/abs/2310.08864">paper</a> |
        <a href="https://robotics-transformer-x.github.io/">website</a> |
        <a href="https://github.com/google-deepmind/open_x_embodiment">code</a> 
      </td>
    </tr>


        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2024-bvs.png" alt="clean-usnob" width="210" height="120">
          </td>
        <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation</papertitle>
          </a>
          <br>
          Yunhao Ge*, Yihe Tang*, Jiashu Xu*, Cem Gokmen*, Chengshu Li, Wensi Ai, Benjamin Jose Martinez, Arman Aydin, Mona Anvari, 
          Ayush K Chakravarthy, Hong-Xing Yu, Josiah Wong, Sanjana Srivastava, Sharon Lee, Shengxin Zha, Laurent Itti, Yunzhu Li, 
          Roberto Martin-Martin, Miao Liu, Pengchuan Zhang, <u>Ruohan Zhang</u>, Li Fei-Fei, Jiajun Wu  (*equally contributed)
          <br>
          <em>Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2024 &nbsp <font color="red"><strong>(Highlight)</strong></font>
          <br>
          <a href="https://twitter.com/RuohanZhang76/status/1802470520493146214">tl;dr</a> |
          <a href="https://arxiv.org/abs/2405.09546">paper</a> |
          <a href="https://behavior-vision-suite.github.io/">website</a> |
          <a href="https://github.com/behavior-vision-suite/behavior-vision-suite.github.io">code</a> 
        </td>
      </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2024-dexcap.png" alt="clean-usnob" width="210" height="140">
          </td>
        <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation</papertitle>
          </a>
          <br>
          Chen Wang, Haochen Shi, Weizhuo Wang, <u>Ruohan Zhang</u>, Li Fei-Fei, C. Karen Liu
          <br>
          <em>Robotics: Science and Systems (RSS)</em>, 2024
          <br>
          <a href="https://twitter.com/RuohanZhang76/status/1769533513940738127">tl;dr</a> |
          <a href="https://arxiv.org/abs/2403.07788">paper</a> |
          <a href="https://dex-cap.github.io/">website</a> |
          <a href="https://docs.google.com/document/d/1ANxSA_PctkqFf3xqAkyktgBgDWEbrFK7b1OnJe54ltw/edit#heading=h.yxlxo67jgfyx">hardware</a> |
          <a href="https://github.com/j96w/DexCap">code</a> 
        </td>
      </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2024-telemoma.png" alt="clean-usnob" width="210" height="140">
          </td>
        <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation</papertitle>
          </a>
          <br>
          Shivin Dass, Wensi Ai, Yuqian Jiang, Samik Singh, Jiaheng Hu, <u>Ruohan Zhang</u>, Peter Stone, Ben Abbatematteo, Roberto Mart√≠n-Mart√≠n
          <br>
          <em>arxiv</em>, 2024
          <br>
          <a href="https://twitter.com/RuohanZhang76/status/1769535561998462982">tl;dr</a> |
          <a href="https://arxiv.org/abs/2403.07869">paper</a> |
          <a href="https://robin-lab.cs.utexas.edu/telemoma-web/">website</a> |
          <a href="https://github.com/UT-Austin-RobIn/telemoma">code</a> 
        </td>
      </tr>


        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023-noir.png" alt="clean-usnob" width="210" height="140">
          </td>
        <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>NOIR: Neural Signal Operated Intelligent Robots for Everyday Activities</papertitle>
          </a>
          <br>
          <u>Ruohan Zhang</u>*, Sharon Lee*, Minjune Hwang*, Ayano Hiranaka*, Chen Wang, Wensi Ai, Jin Jie Ryan Tan, Shreya Gupta, Yilun Hao, Gabrael Levine, Ruohan Gao, Anthony Norcia, Li Fei-Fei, Jiajun Wu (*equally contributed)
          <br>
          <em>Bridging the Gap between Cognitive Science and Robot Learning in the Real World Workshop at CoRL</em>, 2023 &nbsp <font color="red"><strong>(Oral)</strong></font>
          <br>
          <em>Conference on Robot Learning (CoRL)</em>, 2023
          <br>
          <a href="https://twitter.com/RuohanZhang76/status/1720525179028406492">tl;dr</a> |
          <a href="https://arxiv.org/abs/2311.01454">paper</a> |
          <a href="https://noir-corl.github.io/">website</a> | 
          <a href="https://hai.stanford.edu/news/wearable-device-allows-humans-control-robots-brain-waves">Stanford HAI News</a> |
          <a href="https://w.mgtv.com/b/607797/20399189.html?t=videoshare&externalsource=v_play&tc=jXKKosRPSAN7&f=wxf&dc=9c199a33-461e-488e-bc06-3b252fe3e950">MangoTV interview (ÊπñÂçóÂç´ËßÜ‰∏ìËÆø)</a> |
          <a href="https://www.deeplearning.ai/the-batch/issue-249/">The Batch</a> |
          <a href="https://braintitan.medium.com/noir-brain-powered-robots-for-daily-tasks-2cebd6ff6046">Medium</a>
        </td>
      </tr>


        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023-voxposer.png" alt="clean-usnob" width="210" height="140">
          </td>
        <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models</papertitle>
          </a>
          <br>
          Wenlong Huang, Chen Wang, <u>Ruohan Zhang</u>, Yunzhu Li, Jiajun Wu, Li Fei-Fei
          <br>
          <em>Conference on Robot Learning (CoRL)</em>, 2023 &nbsp <font color="red"><strong>(Oral)</strong></font>
          <br>
          <a href="https://arxiv.org/abs/2307.05973">paper</a> |
          <a href="https://voxposer.github.io/">website</a>
        </td>
      </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023-mimicplay.png" alt="clean-usnob" width="210" height="140">
          </td>
        <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>Mimicplay: Long-horizon Imitation Learning by Watching Human Play</papertitle>
          </a>
          <br>
          Chen Wang, Linxi Fan, Jiankai Sun, <u>Ruohan Zhang</u>, Li Fei-Fei, Danfei Xu, Yuke Zhu, Anima Anandkumar
          <br>
          <em>Conference on Robot Learning (CoRL)</em>, 2023 &nbsp <font color="red"><strong>(Finalist - Best Paper/Best Student Paper Awards,
            Finalist - Best Systems Paper Award, Oral)</strong></font>
          <br>
          <a href="https://arxiv.org/abs/2302.12422">paper</a> |
          <a href="https://mimic-play.github.io/">website</a>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/2023-minibh.png" alt="clean-usnob" width="210" height="160">
        </td>
      <td style="padding:0px;width:75%;vertical-align:middle">
          <papertitle>Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI</papertitle>
        </a>
        <br>
        Emily Jin, Jiaheng Hu, Zhuoyi Huang, <u>Ruohan Zhang</u>, Jiajun Wu, Li Fei-Fei, Roberto Mart√≠n-Mart√≠n
        <br>
        <em>Agent Learning in Open-Endedness (ALOE) Workshop at NeurIPS</em>, 2023         <br>
        <em>Generalization in Planning (GenPlan) Workshop at NeurIPS</em>, 2023
        <br>
        <a href="https://arxiv.org/abs/2310.01824">paper</a> |
        <a href="https://github.com/StanfordVL/mini_behavior/">Code</a>
      </td>
    </tr>


      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/2023-vivr.png" alt="clean-usnob" width="210" height="120">
        </td>
      <td style="padding:0px;width:75%;vertical-align:middle">
          <papertitle>Quantifying the Effect of Visual Impairments on Daily Activities in Virtual, Interactive Environments</papertitle>
        </a>
        <br>
        Wensi Ai, Sharon Lee, Li Fei-Fei, Jiajun Wu, <u>Ruohan Zhang</u>
        <br>
        <em>Proceedings of the Annual Meeting of the Cognitive Science Society (CogSci)</em>, 2023
        <br>
        <a href="https://escholarship.org/content/qt2sj3r0n2/qt2sj3r0n2.pdf">paper</a> |
        <a href="https://sites.google.com/view/vi-vr/?authuser=1">website</a>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/2023-seed.png" alt="clean-usnob" width="210" height="130">
      </td>
    <td style="padding:0px;width:75%;vertical-align:middle">
        <papertitle>Primitive Skill-based Robot Learning from Human Evaluative Feedback</papertitle>
      </a>
      <br>
      Ayano Hiranaka*, Minjune Hwang*, Sharon Lee, Chen Wang, Li Fei-Fei, Jiajun Wu, <u>Ruohan Zhang</u> (*equally contributed)
      <br>
      <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2023
      <br>
      <a href="https://arxiv.org/abs/2307.15801">paper</a> |
      <a href="https://seediros23.github.io/">website</a> |
      <a href="https://github.com/mj-hwang/seed">code</a>

    </td>
  </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/2023-finv.png" alt="clean-usnob" width="210" height="120">
        </td>
      <td style="padding:0px;width:75%;vertical-align:middle">
          <papertitle>Partial-View Object View Synthesis via Filtering Inversion</papertitle>
        </a>
        <br>
        Fan-Yun Sun, Jonathan Tremblay, Valts Blukis, Kevin Lin, Danfei Xu, Boris Ivanovic, Peter Karkus, Stan Birchfield, Dieter Fox, <u>Ruohan Zhang</u>, Yunzhu Li, Jiajun Wu, Marco Pavone, Nick Haber
        <br>
        <em>International Conference on 3D Vision (3DV) </em>, 2024 &nbsp <font color="red"><strong>(Oral)</strong></font>
        <br>
        <a href="https://arxiv.org/abs/2304.00673">paper</a> |
        <a href="https://cs.stanford.edu/~sunfanyun/finv/">website</a> |
        <a href="https://github.com/sunfanyunn/FINV">code</a>
      </td>
    </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023-sgm.png" alt="clean-usnob" width="210" height="105">
          </td>
        <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>Modeling Dynamic Environments with Scene Graph Memory</papertitle>
          </a>
          <br>
          Andrey Kurenkov, Michael Lingelbach, Tanmay Agarwal, Chengshu Li, Emily Jin, <u>Ruohan Zhang</u>, Li Fei-Fei, Jiajun Wu, Silvio Savarese, Roberto Mart√≠n-Mart√≠n
          <br>
          <em>International Conference on Machine Learning (ICML)</em>, 2023
          <br>
          <a href="https://proceedings.mlr.press/v202/kurenkov23a.html">paper</a> |
          <a href="https://github.com/andreykurenkov/modeling_env_dynamics">code</a>
        </td>
      </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2023-sgs.png" alt="clean-usnob" width="210" height="140">
          </td>
        <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>Task-Driven Graph Attention for Hierarchical Relational Object Navigation</papertitle>
          </a>
          <br>
          Michael Lingelbach, Chengshu Li, Minjune Hwang, Andrey Kurenkov, Alan Lou, Roberto Mart√≠n-Mart√≠n, <u>Ruohan Zhang</u>, Li Fei-Fei, Jiajun Wu
          <br>
          <em>International Conference on Robotics and Automation (ICRA)</em>, 2023
          <br>
          <a href="https://arxiv.org/abs/2306.13760">paper</a> |
          <a href="https://github.com/mjlbach/ssg">code</a>
        </td>
      </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2022-b1k2.png" alt="clean-usnob" width="210" height="140">
          </td>
        <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and Realistic Simulation</papertitle>
          </a>
          <br>
          Chengshu Li*, <u>Ruohan Zhang</u>*, Josiah Wong*, Cem Gokmen*, Sanjana Srivastava*, Roberto Mart√≠n-Mart√≠n*, Chen Wang*, Gabrael Levine*, Wensi Ai*, Benjamin Martinez, Hang Yin, Michael Lingelbach, Minjune Hwang, Ayano Hiranaka, Sujay Garlanka, Arman Aydin, Sharon Lee, Jiankai Sun, Mona Anvari, Manasi Sharma, Dhruva Bansal, Samuel Hunter, Kyu-Young Kim, Alan Lou, Caleb R Matthews, Ivan Villa-Renteria, Jerry Huayang Tang, Claire Tang, Fei Xia, Yunzhu Li, Silvio Savarese, Hyowon Gweon, C. Karen Liu, Jiajun Wu, Li Fei-Fei          (*equally contributed)
          <br>
          <em>Conference on Robot Learning (CoRL)</em>, 2022 &nbsp <font color="red"><strong>(Best Paper Nominee, Oral)</strong></font>
          <br>
          <a href="https://arxiv.org/abs/2403.09227">paper</a> ÔΩú
          <a href="https://behavior.stanford.edu/">website</a>
        </td>
      </tr>


        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2022-dr-hrl.png" alt="clean-usnob" width="210" height="140">
          </td>
        <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>A Dual Representation Framework for Robot Learning with Human Guidance</papertitle>
          </a>
          <br>
          <u>Ruohan Zhang</u>*, Dhruva Bansal*, Yilun Hao*, Ayano Hiranaka, Jialu Gao, Chen Wang, Roberto Mart√≠n-Mart√≠n, Li Fei-Fei, Jiajun Wu (*equally contributed)
          <br>
          <em>Aligning Robot Representations with Humans Workshop at CoRL</em>, 2022 &nbsp <font color="red"><strong>(Spotlight)</strong></font>
          <br>
          <em>Conference on Robot Learning (CoRL)</em>, 2022
          <br>
          <a href="https://openreview.net/pdf?id=H6rr_CGzV9y">paper</a> |
          <a href="https://openreview.net/attachment?id=H6rr_CGzV9y&name=supplementary_material">appendix</a> |
          <a href="https://sites.google.com/view/dr-hrl">website</a>
        </td>
      </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2022-imma.png" alt="clean-usnob" width="210" height="160">
          </td>
        <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>Interaction Modeling with Multiplex Attention</papertitle>
          </a>
          <br>
          Fan-Yun Sun, Isaac Kauvar, <u>Ruohan Zhang</u>, Jiachen Li, Mykel Kochenderfer, Jiajun Wu, Nick Haber
          <br>
          <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022
          <br>
          <a href="https://openreview.net/forum?id=SeHslYhFx5-">link</a> |
          <a href="https://github.com/fanyun-sun/IMMA">code</a>
        </td>
      </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2022-gradient.png" alt="clean-usnob" width="210" height="160">
          </td>
        <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>How to Train your Decision-Making AIs?</papertitle>
          </a>
          <br>
          <u>Ruohan Zhang</u>, Dhruva Bansal
          <br>
          <em>The Gradient</em>, 2022 
          <br>
          <a href="https://thegradient.pub/how-to-train-your-decision-making-ais/">link</a> | 
          <a href="https://ai.stanford.edu/~zharu/publications/2022_TheGradient_HumanTrainAI.pdf">paper</a>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/2022-attention.png" alt="clean-usnob" width="210" height="160">
        </td>
      <td style="padding:0px;width:75%;vertical-align:middle">
          <papertitle>Selective Visual Attention during Public Speaking in an Immersive Context</papertitle>
        </a>
        <br>
        Mikael Rubin, Sihang Guo, Karl Muller, <u>Ruohan Zhang</u>, Michael Telch, Mary Hayhoe
        <br>
        <em>Attention, Perception, & Psychophysics</em>, 2022 
        <br>
        <a href="https://link.springer.com/article/10.3758/s13414-021-02430-x">link</a> | 
        <a href="https://link.springer.com/content/pdf/10.3758/s13414-021-02430-x.pdf">paper</a> |
        <a href="https://jov.arvojournals.org/article.aspx?articleid=2777608">VSS2021 abstract</a>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/2021-hvma.png" alt="clean-usnob" width="210" height="160">
      </td>
    <td style="padding:0px;width:75%;vertical-align:middle">
        <papertitle>Machine versus Human Attention in Deep Reinforcement Learning Tasks</papertitle>
      </a>
      <br>
      Sihang Guo, <u>Ruohan Zhang</u>, Bo Liu, Yifeng Zhu, Mary Hayhoe, Dana Ballard, Peter Stone
      <br>
      <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021
      <br>
      <a href="https://proceedings.neurips.cc/paper/2021/hash/d58e2f077670f4de9cd7963c857f2534-Abstract.html">link</a> | 
      <a href="https://proceedings.neurips.cc/paper/2021/file/d58e2f077670f4de9cd7963c857f2534-Paper.pdf">paper</a> |
      <a href="https://arxiv.org/abs/2010.15942">arxiv</a> | 
      <a href="https://jov.arvojournals.org/article.aspx?articleid=2776905">VSS2021 abstract</a> 
    </td>
  </tr>

  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/2021-expand.png" alt="clean-usnob" width="210" height="160">
    </td>
  <td style="padding:0px;width:75%;vertical-align:middle">
      <papertitle>Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation</papertitle>
    </a>
    <br>
    Lin Guan, Mudit Verma, Sihang Guo, <u>Ruohan Zhang</u>, Subbarao Kambhampati
    <br>
    <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021 &nbsp <font color="red"><strong>(Spotlight)</strong></font>
    <br>
    <a href="https://proceedings.neurips.cc/paper/2021/hash/b6f8dc086b2d60c5856e4ff517060392-Abstract.html">link</a> | 
    <a href="https://proceedings.neurips.cc/paper/2021/file/b6f8dc086b2d60c5856e4ff517060392-Paper.pdf">paper</a> |
    <a href="https://arxiv.org/pdf/2006.14804.pdf">arxiv</a> |
    <a href="https://papertalk.org/papertalks/37212">talk</a>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2021-guidance.png" alt="clean-usnob" width="210" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>Recent Advances in Leveraging Human Guidance for Sequential Decision-Making tasks</papertitle>
  </a>
  <br>
  <u>Ruohan Zhang</u>*, Faraz Torabi*, Garrett Warnell, Peter Stone. (*equally contributed)
  <br>
  <em>Autonomous Agents and Multi-Agent Systems (JAAMAS)</em>, 2021 
  <br>
  <a href="https://link.springer.com/article/10.1007/s10458-021-09514-w">link</a> | 
  <a href="https://link.springer.com/content/pdf/10.1007/s10458-021-09514-w.pdf">paper</a> |
  <a href="https://arxiv.org/abs/2107.05825">arxiv</a>
</td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/ut-logo2.png" alt="clean-usnob" width="210" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>A Modular Attention Hypothesis for Modeling Visuomotor Behaviors</papertitle>
  </a>
  <br>
  <u>Ruohan Zhang</u>
  <br>
  <em>The University of Texas at Austin Ph.D. Dissertation</em>, 2021 
  <br>
  <a href="https://repositories.lib.utexas.edu/bitstream/handle/2152/86946/ZHANG-DISSERTATION-2021.pdf?sequence=1">paper</a> | 
  <a href="https://ai.stanford.edu/~zharu/publications/disser_slides.pdf">slides</a>
</td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2021-cgl.png" alt="clean-usnob" width="210" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>Efficiently Guiding Imitation Learning Algorithms with Human Gaze</papertitle>
  </a>
  <br>
  Akanksha Saran, <u>Ruohan Zhang</u>, Elaine Schaertl Short, Scott Niekum
  <br>
  <em>Autonomous Agents and Multi-Agent Systems (AAMAS)</em>, 2021
  <br>
  <a href="  https://www.ifaamas.org/Proceedings/aamas2021/pdfs/p1109.pdf">paper</a> |
  <a href="https://arxiv.org/abs/2002.12500">arxiv</a> |
  <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:NvuROmPKKcYJ:scholar.google.com/&output=citation&scisdr=CgXjW_uNEJrjqfCd4zQ:AAGBfm0AAAAAY2eb-zTxayN0mkvTp9uM5tf2xwk4IaTP&scisig=AAGBfm0AAAAAY2eb-xCVkGwRuomTlLG_OuN5LeyPzxk_&scisf=4&ct=citation&cd=-1&hl=en&scfhb=1">bibtex</a> |
  <a href="https://github.com/asaran/IL-CGL">code</a> |
  <a href="https://drive.google.com/file/d/1bvfnkqFGF8O39yaF1QOcvhrOx2CpZyxX/view?usp=sharing">slides</a> |
  <!-- <a href="https://drive.google.com/file/d/1tMnPpqOTG7Jxy34ARQVjAJtSArqLTRw6/view?usp=sharing">spotlight</a> | -->
  <a href="https://techxplore.com/news/2020-03-imitation-algorithms-human.html">media</a>
</td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2021-tics.png" alt="clean-usnob" width="210" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>The Hierarchical Evolution in Human Vision Modeling</papertitle>
  </a>
  <br>
  Dana H. Ballard, <u>Ruohan Zhang</u>
  <br>
  <em>Topics in Cognitive Sciences</em>, 2021 
  <br>
  <a href=https://onlinelibrary.wiley.com/doi/10.1111/tops.12527>link</a> | 
  <a href="https://ai.stanford.edu/~zharu/publications/2020_TiCS_hier.pdf">paper</a>
</td>
</tr>


<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2020-gaze-sruvey.png" alt="clean-usnob" width="190" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>Human Gaze Assisted Artificial Intelligence: A Review</papertitle>
  </a>
  <br>
  <u>Ruohan Zhang</u>, Akanksha Saran, Bo Liu, Yifeng Zhu, Sihang Guo, Scott Niekum, Dana Ballard, Mary Hayhoe
  <br>
  <em>International Joint Conference on Artificial Intelligence (IJCAI) Survey Track</em>, 2020
  <br>
  <a href="https://www.ijcai.org/Proceedings/2020/0689.pdf">paper</a>
</td>
</tr>


<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2020-gamma.png" alt="clean-usnob" width="210" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>Parallel Neural Processing with Gamma Frequency Latencies</papertitle>
  </a>
  <br>
  <u>Ruohan Zhang</u>, Dana H. Ballard
  <br>
  <em>Neural Computation</em>, 2020
  <br>
  <a href="https://www.mitpressjournals.org/doi/full/10.1162/neco_a_01301">link</a> |
  <a href="https://ai.stanford.edu/~zharu/publications/neco_a_01301.pdf">paper</a>
</td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2020-ahead.png" alt="clean-usnob" width="210" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle> Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset</papertitle>
  </a>
  <br>
  <u>Ruohan Zhang</u>, Calen Walshe, Zhuode Liu, Lin Guan, Karl S. Muller, Jake A. Whritner, Luxin Zhang, Mary M Hayhoe, Dana Ballard
  <br>
  <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2020 
  <br>
  <em>AAAI Reinforcement Learning in Games Workshop</em>, 2020 &nbsp <font color="red"><strong>(Oral)</strong></font>
  <br>
  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6161/">link</a> |  
  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6161/6017">paper</a> |
  <a href="https://arxiv.org/abs/1903.06754">arxiv</a> | 
  <a href="https://zenodo.org/record/3451402#.XYPhzOtKhhE">dataset</a> |
  <a href="https://github.com/asaran/IL-CGL/tree/master/BC-CGL">code</a> |
  <a href=http://ai.stanford.edu/~zharu/publications/2020_AAAI_Dataset_poster.pdf>poster</a> |
  <a href=http://ai.stanford.edu/~zharu/publications/AAAI2020-RLG.pdf>AAAI2020 RLG Workshop talk</a>
</td>
</tr>


<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2020-attrl.png" alt="clean-usnob" width="210" height="140">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle> An Initial Attempt of Combining Visual Selective Attention with Deep Reinforcement Learning</papertitle>
  </a>
  <br>
  Liu Yuezhang, <u>Ruohan Zhang</u>, Dana Ballard
  <br>
  <em>arxiv</em>, 2020
  <br>
  <a href="https://arxiv.org/abs/1811.04407">arxiv</a>
</td>
</tr>


<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2019-guidance.png" alt="clean-usnob" width="210" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>Leveraging Human Guidance for Deep Reinforcement Learning Tasks</papertitle>
  </a>
  <br>
  <u>Ruohan Zhang</u>, Faraz Torabi, Lin Guan, Dana Ballard, Peter Stone
  <br>
  <em>International Joint Conference on Artificial Intelligence (IJCAI) Survey Track</em>, 2019
  <br>
  <a href="https://www.ijcai.org/Proceedings/2019/0884.pdf">paper</a> |
  <a href="https://arxiv.org/abs/1909.09906">arxiv</a>
</td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2018-agil2.png" alt="clean-usnob" width="210" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>AGIL: Learning Attention from Human for Visuomotor Tasks</papertitle>
  </a>
  <br>
  <u>Ruohan Zhang</u>, Zhuode Liu, Luxin Zhang, Jake Whritner, Karl Muller, Mary Hayhoe, Dana Ballard
  <br>
  <em>European Conference on Computer Vision (ECCV)</em>, 2018
  <br>
  <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Ruohan_Zhang_AGIL_Learning_Attention_ECCV_2018_paper.html">link</a> |
  <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Ruohan_Zhang_AGIL_Learning_Attention_ECCV_2018_paper.pdf">paper</a> | 
  <a href="https://arxiv.org/abs/1806.03960">arxiv</a> |
  <a href="https://zenodo.org/record/3451402#.XYPhzOtKhhE">dataset</a> |
  <a href="https://github.com/asaran/IL-CGL/tree/master/BC-CGL">code</a> |
  <a href="https://jov.arvojournals.org/article.aspx?articleid=2699524">VSS2018 abstract</a> |
  <a href="https://ai.stanford.edu/~zharu/publications/VSS_2018_Modeling_Complex_Perception-Action_Choices.pdf">VSS2018 talk</a> |
  <a href="https://ccn.societyconference.com/documents/1031/5928d74a68ed3f3c458a258b.pdf">CCN2017 version</a>
</td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2018-mirl.png" alt="clean-usnob" width="210" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>Modeling Sensory-Motor Decisions in Natural Behavior</papertitle>
  </a>
  <br>
  <u>Ruohan Zhang</u>, Shun Zhang, Matthew Tong, Yuchen Cui, Constatin Rothkopf, Dana Ballard, Mary Hayhoe
  <br>
  <em>PLoS Computational Biology</em>, 2018
  <br>
  <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006518&rev=2">link</a> |
  <a href="https://journals.plos.org/ploscompbiol/article/file?rev=2&id=10.1371/journal.pcbi.1006518&type=printable">paper</a> | 
  <a href="https://jov.arvojournals.org/article.aspx?articleid=2652133">VSS2017 abstract</a> |
  <a href="https://ai.stanford.edu/~zharu/publications/2017_VSS_MIRL.pdf">VSS2017 talk</a> |
  <a href="  http://ai.stanford.edu/~zharu/publications/2016_NETI_mirl.pdf">NETI2016 poster</a> |
</td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2018-qest.png" alt="clean-usnob" width="210" height="140">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>Model Checking For Safe Navigation Among Humans</papertitle>
  </a>
  <br>
  Sebastian Junges, Nils Jansen, Joost-Pieter Katoen, Ufuk Topcu, <u>Ruohan Zhang</u>, Mary Hayhoe
  <br>
  <em>International Conference on Quantitative Evaluation of SysTem (QEST)</em>, 2018
  <br>
  <a href="https://link.springer.com/chapter/10.1007/978-3-319-99154-2_13">link</a> |
  <a href="https://link.springer.com/content/pdf/10.1007/978-3-319-99154-2.pdf">paper</a>
</td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/robocup.jpg" alt="clean-usnob" width="210" height="140">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>Fast and Precise Black and White Ball Detection for RoboCup Soccer</papertitle>
  </a>
  <br>
  Jacob Menashe, Josh Kelle, Katie Genter, Josiah Hanna, Elad Liebman, Sanmit Narvekar, <u>Ruohan Zhang</u>, Peter Stone
  <br>
  <em>RoboCup Symposium</em>, 2017
  <br>
  <a href="http://www.cs.utexas.edu/~AustinVilla/papers/LNAI17-jmenashe.pdf">paper</a> |
  <a href="https://spl.robocup.org/open-source/">code</a>
</td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2017-gdmm.png" alt="clean-usnob" width="210" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>Greedy Direction Method of Multiplier for MAP Inference of Large Output Domain</papertitle>
  </a>
  <br>
  Xiangru Huang, Ian E.H. Yen, <u>Ruohan Zhang</u>, Qixing Huang, Pradeep Ravikumar, Inderjit S. Dhillon
  <br>
  <em>Artificial Intelligence and Statistics (AISTATS)</em>, 2017
  <br>
  <a href="http://proceedings.mlr.press/v54/huang17a/huang17a.pdf">paper</a> |
  <a href="https://github.com/xiangruhuang/FastStructPred">code</a><br>
</td>
</tr>



<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2017-art.png" alt="clean-usnob" width="210" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>Participatory Art Museum: Collecting and Modeling Crowd Opinions</papertitle>
  </a>
  <br>
  Xiaoyu Zeng, <u>Ruohan Zhang</u>
  <br>
  <em>AAAI Conference on Artificial Intelligence (AAAI) Student Abstract</em>, 2017
  <br>
  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/11072">link</a> |
  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/11072/10931">paper</a> 

</td>
</tr>


<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2016-robocup.png" alt="clean-usnob" width="210" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>UT Austin Villa: Project-Driven Research in AI and Robotics</papertitle>
  </a>
  <br>
  Katie Genter, Patrick MacAlpine, Jacob Menashe, Josiah Hanna, Elad Liebman, Sanmit Narvekar, <u>Ruohan Zhang</u>, Peter Stone
  <br>
  <em>IEEE Intelligent Systems 31(2)</em>, 2016
  <br>
  <a href="http://ieeexplore.ieee.org/document/7435178/?arnumber=7435178">link</a> |
  <a href="https://spl.robocup.org/open-source/">code</a>
</td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2016-gdmm.png" alt="clean-usnob" width="210" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>Dual Decomposed Learning with Factorwise Oracle for Structural SVM of Large Output Domain</papertitle>
  </a>
  <br>
  Ian E.H. Yen, Xiangru Huang, Kai Zhong, <u>Ruohan Zhang</u>, Pradeep Ravikumar, Inderjit S. Dhillon
  <br>
  <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2016
  <br>
  <a href="http://papers.nips.cc/paper/6422-dual-decomposed-learning-with-factorwise-oracle-for-structural-svm-of-large-output-domain">link</a> |
  <a href="https://proceedings.neurips.cc/paper/2016/file/7e83722522e8aeb7512b7075311316b7-Paper.pdf">paper</a>
</td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2016-cmdp.png" alt="clean-usnob" width="190" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>Decision-Making Policies for Heterogeneous Autonomous Multi-Agent Systems with Safety Constraints</papertitle>
  </a>
  <br>
  <u>Ruohan Zhang</u>, Yue Yu, Mahmoud El Chamie, Beh√ßet A√ßikmese, and Dana H. Ballard
  <br>
  <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2016
  <br>
  <a href=http://www.ijcai.org/Proceedings/16/Papers/084.pdf>paper</a>

</td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/2016-msy.png" alt="clean-usnob" width="210" height="160">
  </td>
<td style="padding:0px;width:75%;vertical-align:middle">
    <papertitle>Maximum Sustainable Yield Problem for Robot Foraging and Construction System</papertitle>
  </a>
  <br>
  <u>Ruohan Zhang</u>, Zhao Song
  <br>
  <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2016
  <br>
  <a href=http://www.ijcai.org/Proceedings/16/Papers/387.pdf>paper</a>
</td>
</tr>

    </LI>
    </br>


        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc.</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/2020-gamma2.png" alt="clean-usnob" width="210" height="120">
            </td>
          <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>Cortical Spikes use Analog Sparse Coding</papertitle>
          </a>
          <br>
          Dana Ballard, <u>Ruohan Zhang</u>, Luc Gentet 
          <br>
          <em>bioRxiv</em>, 2020
          <br>
          <a href="https://www.biorxiv.org/content/biorxiv/early/2020/10/21/2020.10.19.331389.full.pdf">paper</a>
          </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/2019-arl.png" alt="clean-usnob" width="210" height="120">
            </td>
          <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>Attention Guided Imitation Learning and Reinforcement Learning</papertitle>
          </a>
          <br>
          <u>Ruohan Zhang</u>  
          <br>
          <em>AAAI Conference on Artificial Intelligence (AAAI) Doctoral Consortium</em>, 2019
          <br>
          <a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/5090/4963">paper</a> |
          <a href=http://ai.stanford.edu/~zharu/publications/2019_AAAI_DC_poster.pdf>NETI2019 poster</a>
         </td>
         </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/2018-gaze.png" alt="clean-usnob" width="210" height="80">
            </td>
          <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>Learning Attention Model From Human for Visuomotor Tasks</papertitle>
          </a>
          <br>
          Luxin Zhang, <u>Ruohan Zhang</u>, Zhuode Liu, Mary Hayhoe, Dana Ballard
          <br>
          <em>AAAI Conference on Artificial Intelligence (AAAI) Student Abstract</em>, 2018
          <br>
          <a href="https://ojs.aaai.org/index.php/AAAI/article/view/12147">link</a>
          </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/2017-agil.png" alt="clean-usnob" width="210" height="160">
            </td>
          <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>Visual Attention Guided Deep Imitation Learning</papertitle>
          </a>
          <br>
          <u>Ruohan Zhang</u>*, Zhuode Liu*, Luxin Zhang, Karl Muller, Mary Hayhoe, Dana Ballard. (* equally contributed)
          <br>
          <em>NIPS Cognitively Informed Artificial Intelligence Workshop</em>, 2017
          <br>
          <a href="http://ai.stanford.edu/~zharu/publications/2017_NIPS_AGIL.pdf">paper</a>
          </td>
          <br>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/austin-villa.png" alt="clean-usnob" width="210" height="60">
            </td>
          <td style="padding:0px;width:75%;vertical-align:middle">
              <papertitle>UT Austin Villa 2017 Team Description Paper for the Standard Platform League</papertitle>
            </a>
            <br>
            Katie Genter, Josiah Hanna, Josh Kelle, Elad Liebman, Jacob Menashe, Sanmit Narvekar, <u>Ruohan Zhang</u>, Peter Stone
            <br>
            <em>RoboCup Symposium</em>, 2017
            <br>
            <a href="https://pdfwww.gakkai-web.net/gakkai/inter/robocup2017symposium/contents/html/TDPs/soccer_std_plf/UT_TDP17.pdf">paper</a>
          </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/austin-villa.png" alt="clean-usnob" width="210" height="60">
            </td>
          <td style="padding:0px;width:75%;vertical-align:middle">
              <papertitle>UT Austin Villa 2016 Team Description Paper for the Standard Platform League</papertitle>
            </a>
            <br>
            Katie Genter, Josiah Hanna, Josh Kelle, Elad Liebman, Jacob Menashe, Sanmit Narvekar, Rishi Shah, <u>Ruohan Zhang</u>, Peter Stone
            <br>
            <em>RoboCup Symposium</em>, 2016
            <br>
            <a href="http://www.robocup2016.org/media/symposium/Team-Description-Papers/StandardPlatform/RoboCup_2016_SPL_TDP_UTAustinVilla.pdf">paper</a>
          </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/austin-villa.png" alt="clean-usnob" width="210" height="60">
            </td>
          <td style="padding:0px;width:75%;vertical-align:middle">
              <papertitle>UT Austin Villa 2015 Team Description Paper for the Standard Platform League</papertitle>
            </a>
            <br>
            Katie Genter, Josiah Hanna, Elad Liebman, Jacob Menashe, Sanmit Narvekar, Jivko Sinapov, <u>Ruohan Zhang</u>, Peter Stone      
            <br>
            <em>RoboCup Symposium</em>, 2015
            <br>
            <a href="http://robocup2015.oss-cn-shenzhen.aliyuncs.com/TeamDescriptionPapers/StandardPlatform/RoboCup_Symposium_2015_submission_72.pdf">paper</a>
          </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/2015-mrl.png" alt="clean-usnob" width="210" height="140">
            </td>
          <td style="padding:0px;width:75%;vertical-align:middle">
            <papertitle>Global Policy Construction in Modular Reinforcement Learning</papertitle>
          </a>
          <br>
          <u>Ruohan Zhang</u>, Zhao Song, Dana Ballard
           <br>
          <em>AAAI Conference on Artificial Intelligence (AAAI) Student Abstract</em>, 2015
          <br>
          <a href="https://pdfs.semanticscholar.org/1885/be790b8734adefe69f90ea07e0b27ccc08fe.pdf">paper</a>
          </td>
          </tr>

    

    

    
    
 




					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template from  <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
